{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwMRHI9yZ4As"
      },
      "source": [
        "#Trading on Trends [An advanced Sentiment Analysis] - Documentation\n",
        "\n",
        "## Project Requirements and Implementation\n",
        "\n",
        "### Required Criteria\n",
        "\n",
        "#### 1. Using APIs in the project (Criterion 11)\n",
        "   * **Reddit API**: Social media sentiment data collection using PRAW library\n",
        "     - Implementation: `StockSentimentAnalyzer.fetch_reddit_data()`\n",
        "     - Features: Subreddit search, post collection, sentiment scoring\n",
        "   * **Yahoo Finance API**: Historical stock market data retrieval using yfinance\n",
        "     - Implementation: `StockSentimentAnalyzer.fetch_stock_data()`\n",
        "     - Features: OHLCV data, technical indicators, price history\n",
        "   * **Flask API**: Web application endpoints for analysis requests\n",
        "     - Implementation: `/analyze` route in Flask application\n",
        "     - Real-time analysis and visualization generation\n",
        "\n",
        "#### 2. Data cleaning and/or Data transformation (Criterion 3)\n",
        "   * **Text preprocessing**:\n",
        "     - URL removal, special character handling, whitespace normalization\n",
        "     - Implementation: `StockSentimentAnalyzer.clean_text()`\n",
        "   * **Data merging**:\n",
        "     - Stock and sentiment data fusion with date alignment\n",
        "     - Implementation: `StockSentimentAnalyzer.merge_stock_and_sentiment()`\n",
        "   * **Missing value handling**:\n",
        "     - NaN and infinity value replacement with appropriate defaults\n",
        "     - Forward/backward fill for price data, zero-fill for sentiment data\n",
        "   * **Feature engineering**:\n",
        "     - Technical indicators (RSI, MACD, Bollinger Bands)\n",
        "     - Lag features, rolling statistics, interaction features\n",
        "     - Implementation: `StockSentimentAnalyzer.add_technical_indicators()`\n",
        "\n",
        "#### 3. Logistic Regression and variants (Criterion 9)\n",
        "   * **Binary classification** for stock price direction prediction (up/down)\n",
        "   * **Model variants implemented**:\n",
        "     - Standard Logistic Regression with L2 regularization\n",
        "     - Random Forest Classifier\n",
        "     - XGBoost Classifier\n",
        "   * **Implementation**: `StockSentimentAnalyzer.train_models()`\n",
        "   * **Evaluation**: `StockSentimentAnalyzer._evaluate_logistic_model()`\n",
        "     - Metrics: Accuracy, Precision, Recall, F1-score, AUC-ROC\n",
        "     - Confusion matrix visualization\n",
        "\n",
        "### Additional Criteria Implemented\n",
        "\n",
        "#### 1. Object-oriented code (Criterion 1)\n",
        "   * **Main class**: `StockSentimentAnalyzer`\n",
        "   * **Methods organized by functionality**:\n",
        "     - Data collection: `fetch_stock_data()`, `fetch_reddit_data()`\n",
        "     - Data processing: `clean_text()`, `merge_stock_and_sentiment()`\n",
        "     - Analysis: `analyze_data()`, `train_models()`\n",
        "     - Visualization: Private methods for plotting\n",
        "   * **Encapsulation**: Private helper methods, state management\n",
        "\n",
        "#### 2. Regular Expression (Criterion 5)\n",
        "   * **Text cleaning patterns**:\n",
        "     - URL removal: `r'https?://\\S+|www\\.\\S+'`\n",
        "     - Username extraction: `r'@\\w+'`\n",
        "     - Hashtag normalization: `r'#'`\n",
        "     - Special character filtering: `r'[^\\w\\s\\.,!?]'`\n",
        "   * **Implementation**: `StockSentimentAnalyzer.clean_text()`\n",
        "\n",
        "#### 3. Linear Regression and variants (Criterion 8)\n",
        "   * **Continuous target prediction** for stock returns percentage\n",
        "   * **Model variants implemented**:\n",
        "     - Ridge Regression (L2 regularization)\n",
        "     - Gradient Boosting Regressor\n",
        "     - XGBoost Regressor\n",
        "   * **Implementation**: `StockSentimentAnalyzer.train_models()`\n",
        "   * **Evaluation**: `StockSentimentAnalyzer._evaluate_linear_model()`\n",
        "     - Metrics: RMSE, MAE, R², Directional Accuracy\n",
        "\n",
        "### New Features and Enhancements\n",
        "\n",
        "#### 1. Interactive Web Application\n",
        "   * **Flask-based API**: Real-time analysis endpoint\n",
        "   * **Dynamic visualizations**: Plotly charts with interactive elements\n",
        "   * **Multi-panel dashboard**: Price charts, volume, technical indicators\n",
        "\n",
        "#### 2. Advanced Sentiment Analysis\n",
        "   * **Financial lexicon enhancement**: Custom financial terms for VADER\n",
        "   * **Multi-source aggregation**: Combined sentiment from multiple subreddits\n",
        "   * **Temporal features**: Sentiment momentum, rolling averages\n",
        "\n",
        "#### 3. Comprehensive Technical Analysis\n",
        "   * **Indicators implemented**:\n",
        "     - Simple/Exponential Moving Averages (SMA/EMA)\n",
        "     - Relative Strength Index (RSI)\n",
        "     - MACD (Moving Average Convergence Divergence)\n",
        "     - Bollinger Bands\n",
        "     - Average True Range (ATR)\n",
        "     - Stochastic Oscillator\n",
        "   * **Visualization**: Multi-panel technical charts\n",
        "\n",
        "#### 4. Enhanced Model Training Pipeline\n",
        "   * **Feature selection**: Random Forest-based importance ranking\n",
        "   * **Cross-validation**: Time series split for temporal data\n",
        "   * **Class balancing**: SMOTE for handling imbalanced datasets\n",
        "   * **Model selection**: Automated comparison of multiple algorithms\n",
        "\n",
        "#### 5. Robust Error Handling and Data Validation\n",
        "   * **NaN/infinity handling**: Comprehensive cleaning before analysis\n",
        "   * **API error management**: Graceful fallbacks for data collection\n",
        "   * **Visualization safety**: Validated inputs for plotting functions\n",
        "\n",
        "#### 6. Performance Monitoring\n",
        "   * **Model metrics tracking**: JSON storage of evaluation results\n",
        "   * **Feature importance analysis**: Visual comparison across models\n",
        "   * **Backtesting framework**: Historical performance validation\n",
        "\n",
        "#### 7. Scalable Architecture\n",
        "   * **Modular design**: Separate concerns for data, models, visualization\n",
        "   * **Configurable parameters**: Easy adjustment of analysis parameters\n",
        "   * **Extensible framework**: Simple addition of new data sources or models\n",
        "\n",
        "### Project Structure\n",
        "stock_sentiment_analysis/\n",
        "├── data/\n",
        "│   ├── raw/              # Original API data\n",
        "│   ├── processed/        # Cleaned and merged data\n",
        "│   └── final/            # Analysis-ready datasets\n",
        "├── models/               # Trained model artifacts\n",
        "├── visualizations/       # Generated charts and plots\n",
        "├── app.py               # Flask web application\n",
        "├── StockSentimentAnalyzer.py  # Main analysis class\n",
        "└── requirements.txt      # Project dependencies\n",
        "\n",
        "\n",
        "### Future Enhancements\n",
        "1. Real-time streaming data integration\n",
        "2. Advanced deep learning models (LSTM, Transformer)\n",
        "3. Multi-asset portfolio analysis\n",
        "4. Risk management and position sizing\n",
        "5. Deployment on cloud infrastructure\n",
        "6. Mobile-responsive frontend interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw yfinance pandas numpy matplotlib seaborn scikit-learn nltk textblob plotly joblib flask ta xgboost imbalanced-learn pyngrok flask_ngrok"
      ],
      "metadata": {
        "id": "ZX66w7C4WarD",
        "outputId": "082bc7ac-71aa-48f1-db60-9cc2003cc947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.59)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting flask_ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.10.0)\n",
            "Requirement already satisfied: protobuf<6,>=5.29.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.4)\n",
            "Requirement already satisfied: websockets>=11.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=11e3a4d25c53217833afacdcb716ff7cd25d18194ff7a18908a56bc6350fa5f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: pyngrok, update_checker, prawcore, ta, praw, flask_ngrok\n",
            "Successfully installed flask_ngrok-0.0.25 praw-7.8.1 prawcore-2.4.0 pyngrok-7.2.8 ta-0.11.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqTDl8PuhBxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad6a3e5-d926-4747-846f-4e8615961a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.8)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: flask_ngrok in /usr/local/lib/python3.11/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask_ngrok) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask_ngrok) (2.32.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_ngrok) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_ngrok) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_ngrok) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_ngrok) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask_ngrok) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask_ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask_ngrok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask_ngrok) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=0.8->flask_ngrok) (3.0.2)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.59)\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.2.61-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.4)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Downloading yfinance-0.2.61-py2.py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.9/117.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yfinance\n",
            "  Attempting uninstall: yfinance\n",
            "    Found existing installation: yfinance 0.2.59\n",
            "    Uninstalling yfinance-0.2.59:\n",
            "      Successfully uninstalled yfinance-0.2.59\n",
            "Successfully installed yfinance-0.2.61\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n",
        "!pip install flask_ngrok\n",
        "!pip install --upgrade yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIc69gbNfWkH",
        "outputId": "3620264a-facf-4549-e87c-95063f2f12ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2ukQE6toDAkcx1PKiaOSfNTERKP_3W3BmtL46s7pzP9n6QbUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zfxBd5TQ-Bh"
      },
      "outputs": [],
      "source": [
        "# Create directory structure\n",
        "import os\n",
        "os.makedirs(\"data/raw\", exist_ok=True)\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "os.makedirs(\"data/final\", exist_ok=True)\n",
        "os.makedirs(\"visualizations\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **boillinger bands: updated accuracy with XGBoost for R squd**"
      ],
      "metadata": {
        "id": "8IkimM6QM-GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "-z-gM3YW7uEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa49f3c8-e6b1-4954-d6c3-84e3807da7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import yfinance as yf\n",
        "import praw\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix, mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import joblib\n",
        "import json\n",
        "import ta\n",
        "import xgboost as xgb\n",
        "from pyngrok import ngrok\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "# Reddit API Credentials\n",
        "REDDIT_CLIENT_ID = \"XxQG_dz_0xM673dUEE91YA\"\n",
        "REDDIT_CLIENT_SECRET = \"QxDmdmI4IrjMM_3VDBcwk8MWqfJscQ\"\n",
        "REDDIT_USER_AGENT = \"tradingOntrends1.0\"\n",
        "\n",
        "class StockSentimentAnalyzer:\n",
        "    def __init__(self, output_dir=\"data\"):\n",
        "        \"\"\"\n",
        "        Initialize the stock sentiment analyzer\n",
        "\n",
        "        Parameters:\n",
        "        output_dir (str): Directory to save data files\n",
        "        \"\"\"\n",
        "        self.output_dir = output_dir\n",
        "        self.raw_dir = os.path.join(output_dir, \"raw\")\n",
        "        self.processed_dir = os.path.join(output_dir, \"processed\")\n",
        "        self.final_dir = os.path.join(output_dir, \"final\")\n",
        "        self.model_dir = os.path.join(\"models\")\n",
        "        self.viz_dir = os.path.join(\"visualizations\")\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        os.makedirs(self.raw_dir, exist_ok=True)\n",
        "        os.makedirs(self.processed_dir, exist_ok=True)\n",
        "        os.makedirs(self.final_dir, exist_ok=True)\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "        os.makedirs(self.viz_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize sentiment analyzer with enhanced financial lexicon\n",
        "        try:\n",
        "            self.sia = SentimentIntensityAnalyzer()\n",
        "            self._enhance_financial_lexicon()\n",
        "            print(\"VADER sentiment analyzer initialized with financial lexicon\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing VADER: {e}\")\n",
        "            print(\"Will use TextBlob for sentiment analysis\")\n",
        "            self.sia = None\n",
        "\n",
        "        # Initialize Reddit client\n",
        "        try:\n",
        "            self.reddit = praw.Reddit(\n",
        "                client_id=REDDIT_CLIENT_ID,\n",
        "                client_secret=REDDIT_CLIENT_SECRET,\n",
        "                user_agent=REDDIT_USER_AGENT\n",
        "            )\n",
        "            print(\"Reddit client initialized successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing Reddit client: {e}\")\n",
        "            self.reddit = None\n",
        "\n",
        "    def _enhance_financial_lexicon(self):\n",
        "        \"\"\"Add finance-specific terms to VADER lexicon for better sentiment analysis\"\"\"\n",
        "        if not self.sia:\n",
        "            return\n",
        "\n",
        "        # Positive financial terms\n",
        "        positive_terms = {\n",
        "            'bullish': 3.0, 'outperform': 2.5, 'buy': 2.0, 'upgrade': 2.0,\n",
        "            'beat': 1.5, 'exceeds': 1.5, 'growth': 1.0, 'profit': 1.0,\n",
        "            'surge': 1.8, 'rally': 1.7, 'gain': 1.2, 'upside': 1.3,\n",
        "            'momentum': 0.8, 'opportunity': 0.7, 'strong': 0.9, 'higher': 0.7,\n",
        "            'support': 0.6, 'confidence': 0.7, 'positive': 0.8, 'potential': 0.6\n",
        "        }\n",
        "\n",
        "        # Negative financial terms\n",
        "        negative_terms = {\n",
        "            'bearish': -3.0, 'underperform': -2.5, 'sell': -2.0, 'downgrade': -2.0,\n",
        "            'miss': -1.5, 'below': -1.5, 'decline': -1.0, 'loss': -1.0,\n",
        "            'plunge': -1.8, 'crash': -2.5, 'drop': -1.2, 'downside': -1.3,\n",
        "            'weak': -0.9, 'risk': -0.8, 'concern': -0.7, 'lower': -0.7,\n",
        "            'resistance': -0.6, 'recession': -1.5, 'negative': -0.8, 'caution': -0.6\n",
        "        }\n",
        "\n",
        "        # Update the lexicon\n",
        "        self.sia.lexicon.update(positive_terms)\n",
        "        self.sia.lexicon.update(negative_terms)\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        Clean text by removing URLs, special characters, etc.\n",
        "\n",
        "        Parameters:\n",
        "        text (str): Text to clean\n",
        "\n",
        "        Returns:\n",
        "        str: Cleaned text\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "        # Remove usernames\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "        # Remove hashtags symbol (but keep the text)\n",
        "        text = re.sub(r'#', '', text)\n",
        "\n",
        "        # Remove special characters (keep letters, spaces, and basic punctuation)\n",
        "        text = re.sub(r'[^\\w\\s\\.,!?]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def get_sentiment(self, text):\n",
        "        \"\"\"\n",
        "        Get sentiment score using VADER or TextBlob\n",
        "\n",
        "        Parameters:\n",
        "        text (str): Text to analyze\n",
        "\n",
        "        Returns:\n",
        "        dict: Sentiment scores and classification\n",
        "        \"\"\"\n",
        "        cleaned_text = self.clean_text(text)\n",
        "\n",
        "        if not cleaned_text:\n",
        "            return {\n",
        "                'compound': 0,\n",
        "                'positive': 0,\n",
        "                'negative': 0,\n",
        "                'neutral': 0,\n",
        "                'textblob': 0,\n",
        "                'sentiment': 'neutral'\n",
        "            }\n",
        "\n",
        "        # Get TextBlob sentiment\n",
        "        blob_sentiment = TextBlob(cleaned_text).sentiment.polarity\n",
        "\n",
        "        # Get VADER sentiment if available\n",
        "        if self.sia:\n",
        "            vader_scores = self.sia.polarity_scores(cleaned_text)\n",
        "            compound = vader_scores['compound']\n",
        "            positive = vader_scores['pos']\n",
        "            negative = vader_scores['neg']\n",
        "            neutral = vader_scores['neu']\n",
        "        else:\n",
        "            # Fallback to TextBlob for sentiment\n",
        "            compound = blob_sentiment\n",
        "            positive = max(0, compound)\n",
        "            negative = max(0, -compound)\n",
        "            neutral = 1 - (positive + negative)\n",
        "\n",
        "        # Classify sentiment\n",
        "        if compound >= 0.05:\n",
        "            sentiment = 'positive'\n",
        "        elif compound <= -0.05:\n",
        "            sentiment = 'negative'\n",
        "        else:\n",
        "            sentiment = 'neutral'\n",
        "\n",
        "        # Return combined results\n",
        "        return {\n",
        "            'compound': compound,\n",
        "            'positive': positive,\n",
        "            'negative': negative,\n",
        "            'neutral': neutral,\n",
        "            'textblob': blob_sentiment,\n",
        "            'sentiment': sentiment\n",
        "        }\n",
        "\n",
        "    def add_technical_indicators(self, stock_data):\n",
        "        \"\"\"\n",
        "        Add technical analysis indicators to stock data\n",
        "\n",
        "        Parameters:\n",
        "        stock_data (DataFrame): DataFrame with stock price data\n",
        "\n",
        "        Returns:\n",
        "        DataFrame: DataFrame with added technical indicators\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Make a copy to avoid modifying the original\n",
        "            df = stock_data.copy()\n",
        "\n",
        "            # Simple Moving Averages\n",
        "            df['sma_5'] = df['Close'].rolling(window=5).mean()\n",
        "            df['sma_10'] = df['Close'].rolling(window=10).mean()\n",
        "            df['sma_20'] = df['Close'].rolling(window=20).mean()\n",
        "\n",
        "            # Exponential Moving Averages\n",
        "            df['ema_5'] = df['Close'].ewm(span=5, adjust=False).mean()\n",
        "            df['ema_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "            df['ema_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "\n",
        "            # Moving Average Convergence Divergence (MACD)\n",
        "            try:\n",
        "                macd = ta.trend.MACD(df['Close'])\n",
        "                df['macd'] = macd.macd()\n",
        "                df['macd_signal'] = macd.macd_signal()\n",
        "                df['macd_diff'] = macd.macd_diff()\n",
        "            except:\n",
        "                # Calculate MACD manually if ta library fails\n",
        "                df['macd'] = df['Close'].ewm(span=12, adjust=False).mean() - df['Close'].ewm(span=26, adjust=False).mean()\n",
        "                df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
        "                df['macd_diff'] = df['macd'] - df['macd_signal']\n",
        "\n",
        "            # Relative Strength Index (RSI)\n",
        "            try:\n",
        "                df['rsi_14'] = ta.momentum.RSIIndicator(df['Close'], window=14).rsi()\n",
        "            except:\n",
        "                # Simplified RSI calculation if ta library fails\n",
        "                delta = df['Close'].diff()\n",
        "                gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "                loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "                rs = gain / loss\n",
        "                df['rsi_14'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "            # Bollinger Bands\n",
        "            try:\n",
        "                bollinger = ta.volatility.BollingerBands(df['Close'], window=20, window_dev=2)\n",
        "                df['bb_upper'] = bollinger.bollinger_hband()\n",
        "                df['bb_middle'] = bollinger.bollinger_mavg()\n",
        "                df['bb_lower'] = bollinger.bollinger_lband()\n",
        "                df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
        "            except:\n",
        "                # Manual Bollinger Bands calculation\n",
        "                df['bb_middle'] = df['Close'].rolling(window=20).mean()\n",
        "                df['bb_std'] = df['Close'].rolling(window=20).std()\n",
        "                df['bb_upper'] = df['bb_middle'] + 2 * df['bb_std']\n",
        "                df['bb_lower'] = df['bb_middle'] - 2 * df['bb_std']\n",
        "                df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
        "\n",
        "            # Average True Range (ATR) - Volatility indicator\n",
        "            try:\n",
        "                df['atr'] = ta.volatility.AverageTrueRange(df['High'], df['Low'], df['Close'], window=14).average_true_range()\n",
        "            except:\n",
        "                # Simplified ATR calculation\n",
        "                high_low = df['High'] - df['Low']\n",
        "                high_close = np.abs(df['High'] - df['Close'].shift())\n",
        "                low_close = np.abs(df['Low'] - df['Close'].shift())\n",
        "                ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
        "                true_range = np.max(ranges, axis=1)\n",
        "                df['atr'] = true_range.rolling(14).mean()\n",
        "\n",
        "            # Stochastic Oscillator\n",
        "            try:\n",
        "                stoch = ta.momentum.StochasticOscillator(df['High'], df['Low'], df['Close'], window=14, smooth_window=3)\n",
        "                df['stoch_k'] = stoch.stoch()\n",
        "                df['stoch_d'] = stoch.stoch_signal()\n",
        "            except:\n",
        "                # Manual Stochastic calculation\n",
        "                df['stoch_k'] = 100 * (df['Close'] - df['Low'].rolling(window=14).min()) / (df['High'].rolling(window=14).max() - df['Low'].rolling(window=14).min())\n",
        "                df['stoch_d'] = df['stoch_k'].rolling(window=3).mean()\n",
        "\n",
        "            # Rate of Change (ROC)\n",
        "            df['roc_5'] = df['Close'].pct_change(periods=5) * 100\n",
        "            df['roc_10'] = df['Close'].pct_change(periods=10) * 100\n",
        "\n",
        "            # Price rate of change\n",
        "            df['close_pct_change'] = df['Close'].pct_change() * 100\n",
        "            df['volume_pct_change'] = df['Volume'].pct_change() * 100\n",
        "\n",
        "            # Price to SMA ratios\n",
        "            df['price_to_sma5'] = df['Close'] / df['sma_5']\n",
        "            df['price_to_sma20'] = df['Close'] / df['sma_20']\n",
        "\n",
        "            # Crossovers (1 when shorter MA crosses above longer MA, -1 for the opposite, 0 otherwise)\n",
        "            df['ema_5_10_cross'] = np.where(df['ema_5'] > df['ema_10'], 1, np.where(df['ema_5'] < df['ema_10'], -1, 0))\n",
        "            df['ema_10_20_cross'] = np.where(df['ema_10'] > df['ema_20'], 1, np.where(df['ema_10'] < df['ema_20'], -1, 0))\n",
        "\n",
        "            # Add volatility features\n",
        "            df['volatility_daily'] = (df['High'] - df['Low']) / df['Open'] * 100\n",
        "            df['volatility_5d'] = df['volatility_daily'].rolling(window=5).mean()\n",
        "\n",
        "            # Gap features\n",
        "            df['gap'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1) * 100\n",
        "\n",
        "            # Advanced rolling window features\n",
        "            df['close_max_5d'] = df['Close'].rolling(window=5).max()\n",
        "            df['close_min_5d'] = df['Close'].rolling(window=5).min()\n",
        "            df['close_mean_5d'] = df['Close'].rolling(window=5).mean()\n",
        "            df['close_std_5d'] = df['Close'].rolling(window=5).std()\n",
        "\n",
        "            # Sentiment rolling features if available\n",
        "            if 'compound_score_mean' in df.columns:\n",
        "                df['sent_roll_mean_3'] = df['compound_score_mean'].rolling(3).mean()\n",
        "                df['sent_roll_std_3'] = df['compound_score_mean'].rolling(3).std()\n",
        "\n",
        "                # Sentiment momentum\n",
        "                df['sent_momentum'] = df['compound_score_mean'] - df['compound_score_mean'].shift(1)\n",
        "                df['sent_shift1'] = df['compound_score_mean'].shift(1)\n",
        "                df['sent_shift2'] = df['compound_score_mean'].shift(2)\n",
        "\n",
        "                # Interaction features\n",
        "                df['sent_price_interaction'] = df['compound_score_mean'] * df['close_pct_change']\n",
        "                df['sent_volume_interaction'] = df['compound_score_mean'] * df['volume_pct_change']\n",
        "\n",
        "            # Fix the deprecated method warning by using proper fillna methods\n",
        "            # First, backward fill\n",
        "            df = df.bfill()\n",
        "            # Then, fill any remaining NaN values with 0\n",
        "            df = df.fillna(0)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding technical indicators: {e}\")\n",
        "            # Return original data if technical indicators fail\n",
        "            return stock_data\n",
        "            \"\"\"\n",
        "            Add technical analysis indicators to stock data\n",
        "\n",
        "            Parameters:\n",
        "            stock_data (DataFrame): DataFrame with stock price data\n",
        "\n",
        "            Returns:\n",
        "            DataFrame: DataFrame with added technical indicators\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # Make a copy to avoid modifying the original\n",
        "                df = stock_data.copy()\n",
        "\n",
        "                # Simple Moving Averages\n",
        "                df['sma_5'] = df['Close'].rolling(window=5).mean()\n",
        "                df['sma_10'] = df['Close'].rolling(window=10).mean()\n",
        "                df['sma_20'] = df['Close'].rolling(window=20).mean()\n",
        "\n",
        "                # Exponential Moving Averages\n",
        "                df['ema_5'] = df['Close'].ewm(span=5, adjust=False).mean()\n",
        "                df['ema_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "                df['ema_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "\n",
        "                # Moving Average Convergence Divergence (MACD)\n",
        "                try:\n",
        "                    macd = ta.trend.MACD(df['Close'])\n",
        "                    df['macd'] = macd.macd()\n",
        "                    df['macd_signal'] = macd.macd_signal()\n",
        "                    df['macd_diff'] = macd.macd_diff()\n",
        "                except:\n",
        "                    # Calculate MACD manually if ta library fails\n",
        "                    df['macd'] = df['Close'].ewm(span=12, adjust=False).mean() - df['Close'].ewm(span=26, adjust=False).mean()\n",
        "                    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
        "                    df['macd_diff'] = df['macd'] - df['macd_signal']\n",
        "\n",
        "                # Relative Strength Index (RSI)\n",
        "                try:\n",
        "                    df['rsi_14'] = ta.momentum.RSIIndicator(df['Close'], window=14).rsi()\n",
        "                except:\n",
        "                    # Simplified RSI calculation if ta library fails\n",
        "                    delta = df['Close'].diff()\n",
        "                    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "                    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "                    rs = gain / loss\n",
        "                    df['rsi_14'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "                # Bollinger Bands\n",
        "                try:\n",
        "                    bollinger = ta.volatility.BollingerBands(df['Close'], window=20, window_dev=2)\n",
        "                    df['bb_upper'] = bollinger.bollinger_hband()\n",
        "                    df['bb_middle'] = bollinger.bollinger_mavg()\n",
        "                    df['bb_lower'] = bollinger.bollinger_lband()\n",
        "                    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
        "                except:\n",
        "                    # Manual Bollinger Bands calculation\n",
        "                    df['bb_middle'] = df['Close'].rolling(window=20).mean()\n",
        "                    df['bb_std'] = df['Close'].rolling(window=20).std()\n",
        "                    df['bb_upper'] = df['bb_middle'] + 2 * df['bb_std']\n",
        "                    df['bb_lower'] = df['bb_middle'] - 2 * df['bb_std']\n",
        "                    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
        "\n",
        "                # Average True Range (ATR) - Volatility indicator\n",
        "                try:\n",
        "                    df['atr'] = ta.volatility.AverageTrueRange(df['High'], df['Low'], df['Close'], window=14).average_true_range()\n",
        "                except:\n",
        "                    # Simplified ATR calculation\n",
        "                    high_low = df['High'] - df['Low']\n",
        "                    high_close = np.abs(df['High'] - df['Close'].shift())\n",
        "                    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
        "                    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
        "                    true_range = np.max(ranges, axis=1)\n",
        "                    df['atr'] = true_range.rolling(14).mean()\n",
        "\n",
        "                # Stochastic Oscillator\n",
        "                try:\n",
        "                    stoch = ta.momentum.StochasticOscillator(df['High'], df['Low'], df['Close'], window=14, smooth_window=3)\n",
        "                    df['stoch_k'] = stoch.stoch()\n",
        "                    df['stoch_d'] = stoch.stoch_signal()\n",
        "                except:\n",
        "                    # Manual Stochastic calculation\n",
        "                    df['stoch_k'] = 100 * (df['Close'] - df['Low'].rolling(window=14).min()) / (df['High'].rolling(window=14).max() - df['Low'].rolling(window=14).min())\n",
        "                    df['stoch_d'] = df['stoch_k'].rolling(window=3).mean()\n",
        "\n",
        "                # Rate of Change (ROC)\n",
        "                df['roc_5'] = df['Close'].pct_change(periods=5) * 100\n",
        "                df['roc_10'] = df['Close'].pct_change(periods=10) * 100\n",
        "\n",
        "                # Price rate of change\n",
        "                df['close_pct_change'] = df['Close'].pct_change() * 100\n",
        "                df['volume_pct_change'] = df['Volume'].pct_change() * 100\n",
        "\n",
        "                # Price to SMA ratios\n",
        "                df['price_to_sma5'] = df['Close'] / df['sma_5']\n",
        "                df['price_to_sma20'] = df['Close'] / df['sma_20']\n",
        "\n",
        "                # Crossovers (1 when shorter MA crosses above longer MA, -1 for the opposite, 0 otherwise)\n",
        "                df['ema_5_10_cross'] = np.where(df['ema_5'] > df['ema_10'], 1, np.where(df['ema_5'] < df['ema_10'], -1, 0))\n",
        "                df['ema_10_20_cross'] = np.where(df['ema_10'] > df['ema_20'], 1, np.where(df['ema_10'] < df['ema_20'], -1, 0))\n",
        "\n",
        "                # Add volatility features\n",
        "                df['volatility_daily'] = (df['High'] - df['Low']) / df['Open'] * 100\n",
        "                df['volatility_5d'] = df['volatility_daily'].rolling(window=5).mean()\n",
        "\n",
        "                # Gap features\n",
        "                df['gap'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1) * 100\n",
        "\n",
        "                # Advanced rolling window features\n",
        "                df['close_max_5d'] = df['Close'].rolling(window=5).max()\n",
        "                df['close_min_5d'] = df['Close'].rolling(window=5).min()\n",
        "                df['close_mean_5d'] = df['Close'].rolling(window=5).mean()\n",
        "                df['close_std_5d'] = df['Close'].rolling(window=5).std()\n",
        "\n",
        "                # Sentiment rolling features if available\n",
        "                if 'compound_score_mean' in df.columns:\n",
        "                    df['sent_roll_mean_3'] = df['compound_score_mean'].rolling(3).mean()\n",
        "                    df['sent_roll_std_3'] = df['compound_score_mean'].rolling(3).std()\n",
        "\n",
        "                    # Sentiment momentum\n",
        "                    df['sent_momentum'] = df['compound_score_mean'] - df['compound_score_mean'].shift(1)\n",
        "                    df['sent_shift1'] = df['compound_score_mean'].shift(1)\n",
        "                    df['sent_shift2'] = df['compound_score_mean'].shift(2)\n",
        "\n",
        "                    # Interaction features\n",
        "                    df['sent_price_interaction'] = df['compound_score_mean'] * df['close_pct_change']\n",
        "                    df['sent_volume_interaction'] = df['compound_score_mean'] * df['volume_pct_change']\n",
        "\n",
        "                # Fill NaN values created by technical indicators\n",
        "                df = df.fillna(method='bfill')\n",
        "                df = df.fillna(0)  # Fill any remaining NaN values\n",
        "\n",
        "                return df\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error adding technical indicators: {e}\")\n",
        "                # Return original data if technical indicators fail\n",
        "                return stock_data\n",
        "\n",
        "    def fetch_stock_data(self, ticker_symbol, period=\"1mo\", interval=\"1d\"):\n",
        "        \"\"\"\n",
        "        Fetch stock data using Yahoo Finance\n",
        "\n",
        "        Parameters:\n",
        "        ticker_symbol (str): Stock ticker symbol\n",
        "        period (str): Period to fetch data for (e.g., \"1mo\", \"3mo\", \"1y\")\n",
        "        interval (str): Data interval (e.g., \"1d\", \"1h\")\n",
        "\n",
        "        Returns:\n",
        "        DataFrame: Stock price data\n",
        "        \"\"\"\n",
        "        print(f\"Fetching stock data for {ticker_symbol}...\")\n",
        "\n",
        "        try:\n",
        "            # Get stock data from Yahoo Finance\n",
        "            stock = yf.Ticker(ticker_symbol)\n",
        "            stock_data = stock.history(period=period, interval=interval)\n",
        "\n",
        "            if stock_data.empty:\n",
        "                print(f\"No data found for {ticker_symbol}\")\n",
        "                return None\n",
        "\n",
        "            # Add datetime index as a column\n",
        "            stock_data['Date'] = stock_data.index\n",
        "\n",
        "            # Calculate daily returns\n",
        "            stock_data['daily_return'] = stock_data['Close'].pct_change() * 100\n",
        "\n",
        "            # Calculate target variable: 1 if price goes up next day, 0 otherwise\n",
        "            stock_data['price_up_next_day'] = stock_data['daily_return'].shift(-1) > 0\n",
        "            stock_data['price_up_next_day'] = stock_data['price_up_next_day'].astype(int)\n",
        "\n",
        "            # Add technical indicators\n",
        "            stock_data = self.add_technical_indicators(stock_data)\n",
        "\n",
        "            # Save to CSV\n",
        "            csv_filename = os.path.join(self.raw_dir, f\"{ticker_symbol}_stock.csv\")\n",
        "            stock_data.to_csv(csv_filename)\n",
        "            print(f\"✅ Stock data saved to {csv_filename}\")\n",
        "\n",
        "            return stock_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching stock data for {ticker_symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_reddit_data(self, ticker_symbol, subreddits=None, limit=100, days_back=30):\n",
        "        \"\"\"\n",
        "        Fetch Reddit posts related to a stock ticker\n",
        "\n",
        "        Parameters:\n",
        "        ticker_symbol (str): Stock ticker symbol\n",
        "        subreddits (list): List of subreddit names to search (default: [\"stocks\", \"investing\", \"wallstreetbets\"])\n",
        "        limit (int): Maximum number of posts to collect per subreddit\n",
        "        days_back (int): Number of days to look back\n",
        "\n",
        "        Returns:\n",
        "        DataFrame: Processed Reddit data with sentiment\n",
        "        \"\"\"\n",
        "        if self.reddit is None:\n",
        "            print(\"Reddit client not initialized. Cannot fetch Reddit data.\")\n",
        "            return None\n",
        "\n",
        "        if subreddits is None:\n",
        "            subreddits = [\"stocks\", \"investing\", \"wallstreetbets\", \"stockmarket\"]\n",
        "\n",
        "        print(f\"Fetching Reddit data for {ticker_symbol} from {subreddits}...\")\n",
        "\n",
        "        all_posts = []\n",
        "        past_date = datetime.now() - timedelta(days=days_back)\n",
        "\n",
        "        for subreddit_name in subreddits:\n",
        "            try:\n",
        "                print(f\"Searching r/{subreddit_name} for posts about {ticker_symbol}...\")\n",
        "                subreddit = self.reddit.subreddit(subreddit_name)\n",
        "                search_query = ticker_symbol\n",
        "\n",
        "                # Search for posts containing the ticker symbol\n",
        "                for post in subreddit.search(search_query, limit=limit):\n",
        "                    # Skip posts older than days_back\n",
        "                    post_date = datetime.fromtimestamp(post.created_utc)\n",
        "                    if post_date < past_date:\n",
        "                        continue\n",
        "\n",
        "                    # Combine title and text for content\n",
        "                    content = f\"{post.title} {post.selftext}\"\n",
        "                    cleaned_content = self.clean_text(content)\n",
        "\n",
        "                    # Get sentiment scores\n",
        "                    sentiment = self.get_sentiment(content)\n",
        "\n",
        "                    all_posts.append({\n",
        "                        'subreddit': subreddit_name,\n",
        "                        'title': post.title,\n",
        "                        'content': content,\n",
        "                        'cleaned_content': cleaned_content,\n",
        "                        'upvotes': post.score,\n",
        "                        'upvote_ratio': post.upvote_ratio,\n",
        "                        'num_comments': post.num_comments,\n",
        "                        'created_at': post_date,\n",
        "                        'author': str(post.author),\n",
        "                        'compound_score': sentiment['compound'],\n",
        "                        'positive_score': sentiment['positive'],\n",
        "                        'negative_score': sentiment['negative'],\n",
        "                        'neutral_score': sentiment['neutral'],\n",
        "                        'textblob_score': sentiment['textblob'],\n",
        "                        'sentiment': sentiment['sentiment'],\n",
        "                        'ticker': ticker_symbol\n",
        "                    })\n",
        "\n",
        "                print(f\"Found {len(all_posts)} posts about {ticker_symbol} in r/{subreddit_name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching data from r/{subreddit_name}: {e}\")\n",
        "\n",
        "        if not all_posts:\n",
        "            print(f\"No Reddit posts found for {ticker_symbol}\")\n",
        "            return None\n",
        "\n",
        "        # Create DataFrame\n",
        "        reddit_df = pd.DataFrame(all_posts)\n",
        "\n",
        "        # Save to CSV\n",
        "        csv_filename = os.path.join(self.raw_dir, f\"{ticker_symbol}_reddit.csv\")\n",
        "        reddit_df.to_csv(csv_filename, index=False)\n",
        "        print(f\"✅ Reddit data saved to {csv_filename}\")\n",
        "\n",
        "        return reddit_df\n",
        "\n",
        "    def merge_stock_and_sentiment(self, ticker_symbol):\n",
        "        \"\"\"\n",
        "        Merge stock data with social media sentiment data\n",
        "\n",
        "        Parameters:\n",
        "        ticker_symbol (str): Stock ticker symbol\n",
        "\n",
        "        Returns:\n",
        "        DataFrame: Merged data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load stock data\n",
        "            stock_file = os.path.join(self.raw_dir, f\"{ticker_symbol}_stock.csv\")\n",
        "            if not os.path.exists(stock_file):\n",
        "                print(f\"Stock data file not found: {stock_file}\")\n",
        "                return None\n",
        "\n",
        "            # Read stock data and handle datetime conversion safely\n",
        "            stock_df = pd.read_csv(stock_file)\n",
        "            try:\n",
        "                # Convert to datetime with explicit UTC handling\n",
        "                stock_df['Date'] = pd.to_datetime(stock_df['Date'], utc=True)\n",
        "                stock_df['date_only'] = stock_df['Date'].dt.date\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting stock dates for {ticker_symbol}: {e}\")\n",
        "                return None\n",
        "\n",
        "            # Load Reddit data if available\n",
        "            reddit_file = os.path.join(self.raw_dir, f\"{ticker_symbol}_reddit.csv\")\n",
        "            if os.path.exists(reddit_file):\n",
        "                reddit_df = pd.read_csv(reddit_file)\n",
        "                try:\n",
        "                    # Convert to datetime with explicit UTC handling\n",
        "                    reddit_df['created_at'] = pd.to_datetime(reddit_df['created_at'], utc=True)\n",
        "                    reddit_df['date_only'] = reddit_df['created_at'].dt.date\n",
        "                except Exception as e:\n",
        "                    print(f\"Error converting Reddit dates for {ticker_symbol}: {e}\")\n",
        "                    return None\n",
        "\n",
        "                # Aggregate Reddit sentiment by date with enhanced metrics\n",
        "                daily_sentiment = reddit_df.groupby('date_only').agg({\n",
        "                    'compound_score': ['mean', 'count', 'std', 'min', 'max'],\n",
        "                    'positive_score': ['mean', 'sum'],\n",
        "                    'negative_score': ['mean', 'sum'],\n",
        "                    'neutral_score': 'mean',\n",
        "                    'textblob_score': ['mean', 'std'],\n",
        "                    'upvotes': ['sum', 'mean'],\n",
        "                    'num_comments': ['sum', 'mean'],\n",
        "                    'upvote_ratio': 'mean'\n",
        "                }).reset_index()\n",
        "\n",
        "                # Flatten multi-level columns\n",
        "                daily_sentiment.columns = ['_'.join(col).strip('_') for col in daily_sentiment.columns.values]\n",
        "\n",
        "                # Rename columns for clarity\n",
        "                daily_sentiment = daily_sentiment.rename(columns={\n",
        "                    'date_only_': 'date_only',\n",
        "                    'compound_score_count': 'post_count'\n",
        "                })\n",
        "\n",
        "                # Calculate sentiment bias (difference between positive and negative)\n",
        "                daily_sentiment['sentiment_bias'] = daily_sentiment['positive_score_sum'] - daily_sentiment['negative_score_sum']\n",
        "\n",
        "                # Calculate sentiment dispersion (ratio of standard deviation to mean)\n",
        "                daily_sentiment['sentiment_dispersion'] = np.abs(daily_sentiment['compound_score_std'] /\n",
        "                                                        (daily_sentiment['compound_score_mean'] + 0.001))\n",
        "\n",
        "                # Calculate engagement ratio (comments per post)\n",
        "                daily_sentiment['engagement_ratio'] = daily_sentiment['num_comments_sum'] / (daily_sentiment['post_count'] + 1)\n",
        "\n",
        "                # Merge with stock data\n",
        "                merged_df = pd.merge(\n",
        "                    stock_df,\n",
        "                    daily_sentiment,\n",
        "                    on='date_only',\n",
        "                    how='left'\n",
        "                )\n",
        "\n",
        "                # Fill NaN sentiment values\n",
        "                sentiment_columns = merged_df.columns[merged_df.columns.str.contains('score|post_count|upvotes|comments|sentiment|engagement')]\n",
        "                merged_df[sentiment_columns] = merged_df[sentiment_columns].fillna(0)\n",
        "\n",
        "            else:\n",
        "                print(f\"No Reddit data found for {ticker_symbol}, using stock data only\")\n",
        "                merged_df = stock_df\n",
        "\n",
        "            # Verify data integrity\n",
        "            if merged_df.empty:\n",
        "                print(f\"No valid data after merging for {ticker_symbol}\")\n",
        "                return None\n",
        "\n",
        "            # Save merged data\n",
        "            output_file = os.path.join(self.processed_dir, f\"{ticker_symbol}_merged.csv\")\n",
        "            merged_df.to_csv(output_file, index=False)\n",
        "            print(f\"✅ Merged data saved to {output_file}\")\n",
        "\n",
        "            return merged_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in merge_stock_and_sentiment for {ticker_symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analyze_data(self, ticker_symbol):\n",
        "        \"\"\"\n",
        "        Analyze the merged data and create visualizations\n",
        "\n",
        "        Parameters:\n",
        "        ticker_symbol (str): Stock ticker symbol\n",
        "\n",
        "        Returns:\n",
        "        dict: Analysis statistics\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load merged data\n",
        "            merged_file = os.path.join(self.processed_dir, f\"{ticker_symbol}_merged.csv\")\n",
        "            if not os.path.exists(merged_file):\n",
        "                print(f\"Merged data file not found: {merged_file}\")\n",
        "                return None\n",
        "\n",
        "            merged_df = pd.read_csv(merged_file)\n",
        "\n",
        "            # Handle datetime conversion safely\n",
        "            try:\n",
        "                merged_df['Date'] = pd.to_datetime(merged_df['Date'], utc=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting dates in analysis for {ticker_symbol}: {e}\")\n",
        "                return None\n",
        "\n",
        "            print(f\"Analyzing data for {ticker_symbol}...\")\n",
        "\n",
        "            # ===== CRITICAL FIX: Handle NaN values before any analysis =====\n",
        "            # Fix NaN values in important columns to fix the \"Input y contains NaN\" error\n",
        "            columns_to_check = ['daily_return', 'compound_score_mean', 'Close', 'Open', 'High', 'Low', 'Volume']\n",
        "            for col in columns_to_check:\n",
        "                if col in merged_df.columns and merged_df[col].isna().any():\n",
        "                    print(f\"Found {merged_df[col].isna().sum()} NaN values in {col}, filling appropriately\")\n",
        "                    if col in ['daily_return', 'compound_score_mean']:\n",
        "                        # For sentiment and returns, fill with 0\n",
        "                        merged_df[col] = merged_df[col].fillna(0)\n",
        "                    elif col in ['Close', 'Open', 'High', 'Low']:\n",
        "                        # For price data, forward fill then backward fill\n",
        "                        merged_df[col] = merged_df[col].ffill().bfill()\n",
        "                    elif col == 'Volume':\n",
        "                        # For volume, fill with median\n",
        "                        merged_df[col] = merged_df[col].fillna(merged_df[col].median())\n",
        "\n",
        "            # ===== Statistical analysis (create early to avoid another error) =====\n",
        "            stats_file = os.path.join(self.processed_dir, f\"{ticker_symbol}_stats.json\")\n",
        "            stats = {\n",
        "                'ticker': ticker_symbol,\n",
        "                'data_points': len(merged_df),\n",
        "                'date_range': [merged_df['Date'].min().strftime('%Y-%m-%d'),\n",
        "                              merged_df['Date'].max().strftime('%Y-%m-%d')],\n",
        "                'avg_close': float(merged_df['Close'].mean()),\n",
        "                'min_close': float(merged_df['Close'].min()),\n",
        "                'max_close': float(merged_df['Close'].max()),\n",
        "                'stddev_close': float(merged_df['Close'].std()),\n",
        "                'avg_volume': float(merged_df['Volume'].mean()),\n",
        "                'avg_daily_return': float(merged_df['daily_return'].mean()),\n",
        "                'stddev_daily_return': float(merged_df['daily_return'].std()),\n",
        "                'up_days_pct': float((merged_df['daily_return'] > 0).mean() * 100)\n",
        "            }\n",
        "\n",
        "            # Add technical indicator statistics if available\n",
        "            if 'rsi_14' in merged_df.columns:\n",
        "                stats.update({\n",
        "                    'avg_rsi': float(merged_df['rsi_14'].mean()),\n",
        "                    'overbought_days_pct': float((merged_df['rsi_14'] > 70).mean() * 100),\n",
        "                    'oversold_days_pct': float((merged_df['rsi_14'] < 30).mean() * 100)\n",
        "                })\n",
        "\n",
        "            if 'volatility_daily' in merged_df.columns:\n",
        "                stats.update({\n",
        "                    'avg_volatility': float(merged_df['volatility_daily'].mean()),\n",
        "                    'max_volatility': float(merged_df['volatility_daily'].max())\n",
        "                })\n",
        "\n",
        "            if 'compound_score_mean' in merged_df.columns:\n",
        "                # Add sentiment statistics\n",
        "                stats.update({\n",
        "                    'avg_sentiment': float(merged_df['compound_score_mean'].mean()),\n",
        "                    'min_sentiment': float(merged_df['compound_score_mean'].min()),\n",
        "                    'max_sentiment': float(merged_df['compound_score_mean'].max()),\n",
        "                    'stddev_sentiment': float(merged_df['compound_score_mean'].std()),\n",
        "                    'positive_days_pct': float((merged_df['compound_score_mean'] > 0).mean() * 100),\n",
        "                    'avg_posts_per_day': float(merged_df['post_count'].mean() if 'post_count' in merged_df.columns else 0)\n",
        "                })\n",
        "\n",
        "                # Safely calculate correlation between sentiment and returns\n",
        "                # This is where NaN values can cause problems\n",
        "                if 'daily_return' in merged_df.columns:\n",
        "                    # Create a temporary dataframe with no NaN values for correlation calculation\n",
        "                    temp_df = merged_df[['compound_score_mean', 'daily_return']].copy()\n",
        "                    temp_df['next_day_return'] = temp_df['daily_return'].shift(-1)\n",
        "                    temp_df = temp_df.dropna()\n",
        "\n",
        "                    if len(temp_df) > 2:  # Need at least 3 points for correlation\n",
        "                        corr = temp_df['compound_score_mean'].corr(temp_df['next_day_return'])\n",
        "                        stats.update({\n",
        "                            'sentiment_return_corr': float(corr)\n",
        "                        })\n",
        "                    else:\n",
        "                        stats.update({\n",
        "                            'sentiment_return_corr': 0.0\n",
        "                        })\n",
        "\n",
        "                # Add engagement statistics if available\n",
        "                if 'engagement_ratio' in merged_df.columns:\n",
        "                    stats.update({\n",
        "                        'avg_engagement': float(merged_df['engagement_ratio'].mean()),\n",
        "                        'max_engagement': float(merged_df['engagement_ratio'].max())\n",
        "                    })\n",
        "\n",
        "            # ===== Plot 1: Stock price and sentiment over time =====\n",
        "            if 'compound_score_mean' in merged_df.columns:\n",
        "                try:\n",
        "                    fig = plt.figure(figsize=(14, 10))\n",
        "\n",
        "                    # Create 3 vertically stacked subplots\n",
        "                    gs = fig.add_gridspec(3, 1, height_ratios=[3, 1, 1], hspace=0.1)\n",
        "\n",
        "                    # Stock price subplot\n",
        "                    ax1 = fig.add_subplot(gs[0])\n",
        "                    ax1.set_title(f'{ticker_symbol} Stock Price vs. Reddit Sentiment', fontsize=14)\n",
        "                    ax1.plot(merged_df['Date'], merged_df['Close'], color='blue', linewidth=2, label='Close Price')\n",
        "                    ax1.set_ylabel('Stock Price ($)', color='blue', fontsize=12)\n",
        "                    ax1.tick_params(axis='y', labelcolor='blue')\n",
        "                    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Add sentiment line on secondary y-axis\n",
        "                    ax2 = ax1.twinx()\n",
        "                    ax2.plot(merged_df['Date'], merged_df['compound_score_mean'], color='green',\n",
        "                          linestyle='--', linewidth=2, label='Sentiment Score')\n",
        "                    ax2.set_ylabel('Sentiment Score', color='green', fontsize=12)\n",
        "                    ax2.tick_params(axis='y', labelcolor='green')\n",
        "\n",
        "                    # Create legend with both price and sentiment\n",
        "                    lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "                    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "                    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "                    # Volume subplot\n",
        "                    ax3 = fig.add_subplot(gs[1], sharex=ax1)\n",
        "                    ax3.bar(merged_df['Date'], merged_df['Volume'], color='gray', alpha=0.6, label='Volume')\n",
        "                    ax3.set_ylabel('Volume', color='gray', fontsize=12)\n",
        "                    ax3.tick_params(axis='y', labelcolor='gray')\n",
        "                    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Sentiment and post count subplot\n",
        "                    ax4 = fig.add_subplot(gs[2], sharex=ax1)\n",
        "                    if 'post_count' in merged_df.columns:\n",
        "                        ax4.bar(merged_df['Date'], merged_df['post_count'], color='orange', alpha=0.6, label='Post Count')\n",
        "                        ax4.set_ylabel('Post Count', color='orange', fontsize=12)\n",
        "                        ax4.tick_params(axis='y', labelcolor='orange')\n",
        "                        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Remove x-axis labels for upper subplots\n",
        "                    ax1.set_xticklabels([])\n",
        "                    ax3.set_xticklabels([])\n",
        "                    ax4.set_xlabel('Date', fontsize=12)\n",
        "\n",
        "                    plt.savefig(os.path.join(self.viz_dir, f\"{ticker_symbol}_price_vs_sentiment.png\"))\n",
        "                    plt.close(fig)  # Explicitly close the figure\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating price vs sentiment plot: {e}\")\n",
        "\n",
        "                # Also create interactive Plotly version\n",
        "                try:\n",
        "                    fig = go.Figure()\n",
        "\n",
        "                    # Add price candlestick\n",
        "                    fig.add_trace(\n",
        "                        go.Candlestick(\n",
        "                            x=merged_df['Date'],\n",
        "                            open=merged_df['Open'],\n",
        "                            high=merged_df['High'],\n",
        "                            low=merged_df['Low'],\n",
        "                            close=merged_df['Close'],\n",
        "                            name='OHLC',\n",
        "                            increasing_line_color='green',\n",
        "                            decreasing_line_color='red'\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    # Add moving averages if available\n",
        "                    if 'sma_20' in merged_df.columns:\n",
        "                        fig.add_trace(\n",
        "                            go.Scatter(\n",
        "                                x=merged_df['Date'],\n",
        "                                y=merged_df['sma_20'],\n",
        "                                name='20-day MA',\n",
        "                                line=dict(color='purple', width=1)\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                    # Add sentiment overlay\n",
        "                    fig.add_trace(\n",
        "                        go.Scatter(\n",
        "                            x=merged_df['Date'],\n",
        "                            y=merged_df['compound_score_mean'],\n",
        "                            name='Sentiment Score',\n",
        "                            line=dict(color='green', width=2, dash='dash'),\n",
        "                            yaxis='y2'\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    # Add RSI if available\n",
        "                    if 'rsi_14' in merged_df.columns:\n",
        "                        fig.add_trace(\n",
        "                            go.Scatter(\n",
        "                                x=merged_df['Date'],\n",
        "                                y=merged_df['rsi_14'],\n",
        "                                name='RSI (14)',\n",
        "                                line=dict(color='orange', width=1),\n",
        "                                visible='legendonly',\n",
        "                                yaxis='y3'\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                    # Configure layout with multiple y-axes\n",
        "                    fig.update_layout(\n",
        "                        title=f'{ticker_symbol} Stock Price vs. Reddit Sentiment',\n",
        "                        xaxis_title='Date',\n",
        "                        yaxis_title='Stock Price ($)',\n",
        "                        yaxis2=dict(\n",
        "                            title='Sentiment Score',\n",
        "                            titlefont=dict(color='green'),\n",
        "                            tickfont=dict(color='green'),\n",
        "                            overlaying='y',\n",
        "                            side='right',\n",
        "                            range=[-1, 1]\n",
        "                        ),\n",
        "                        yaxis3=dict(\n",
        "                            title='RSI',\n",
        "                            titlefont=dict(color='orange'),\n",
        "                            tickfont=dict(color='orange'),\n",
        "                            anchor='free',\n",
        "                            overlaying='y',\n",
        "                            side='right',\n",
        "                            position=0.95,\n",
        "                            range=[0, 100],\n",
        "                            showgrid=False\n",
        "                        ),\n",
        "                        legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.8)'),\n",
        "                        template='plotly_white',\n",
        "                        margin=dict(l=50, r=70, t=50, b=50),\n",
        "                        height=700\n",
        "                    )\n",
        "\n",
        "                    fig.write_html(os.path.join(self.viz_dir, f\"{ticker_symbol}_interactive_analysis.html\"))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating interactive plot: {e}\")\n",
        "\n",
        "            # ===== Plot 2: Trading volume and social media activity =====\n",
        "            if 'post_count' in merged_df.columns:\n",
        "                try:\n",
        "                    fig = plt.figure(figsize=(14, 6))\n",
        "\n",
        "                    ax1 = plt.gca()\n",
        "                    ax1.set_xlabel('Date')\n",
        "                    ax1.set_ylabel('Trading Volume', color='blue')\n",
        "                    ax1.bar(merged_df['Date'], merged_df['Volume'], color='blue', alpha=0.5)\n",
        "                    ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "                    ax2 = ax1.twinx()\n",
        "                    ax2.set_ylabel('Number of Reddit Posts', color='red')\n",
        "                    ax2.plot(merged_df['Date'], merged_df['post_count'], color='red', marker='o', linestyle='-')\n",
        "                    ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "                    plt.title(f'{ticker_symbol} Trading Volume vs. Reddit Activity')\n",
        "                    plt.grid(True, alpha=0.3)\n",
        "                    plt.savefig(os.path.join(self.viz_dir, f\"{ticker_symbol}_volume_vs_activity.png\"))\n",
        "                    plt.close(fig)  # Explicitly close the figure\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating volume vs activity plot: {e}\")\n",
        "\n",
        "            # ===== Plot 3: Correlation between sentiment and next day returns =====\n",
        "            if 'compound_score_mean' in merged_df.columns:\n",
        "                try:\n",
        "                    fig = plt.figure(figsize=(10, 8))\n",
        "\n",
        "                    # Create a clean DataFrame for the scatter plot\n",
        "                    plot_df = merged_df[['Date', 'compound_score_mean', 'daily_return']].copy()\n",
        "\n",
        "                    # Create next day return (shift current day returns back one day)\n",
        "                    plot_df['next_day_return'] = plot_df['daily_return'].shift(-1)\n",
        "\n",
        "                    # Drop any rows with NaN values - THIS IS THE CRITICAL FIX\n",
        "                    plot_df = plot_df.dropna(subset=['compound_score_mean', 'next_day_return'])\n",
        "\n",
        "                    # Only create plot if we have enough data points\n",
        "                    if len(plot_df) >= 3:\n",
        "                        # Add post_count if available\n",
        "                        if 'post_count' in merged_df.columns:\n",
        "                            # Join the post_count column\n",
        "                            plot_df = pd.merge(\n",
        "                                plot_df,\n",
        "                                merged_df[['Date', 'post_count']],\n",
        "                                on='Date',\n",
        "                                how='left'\n",
        "                            )\n",
        "                            # Fill NaN values with a default size value (IMPORTANT FIX)\n",
        "                            plot_df['post_count'] = plot_df['post_count'].fillna(1)\n",
        "\n",
        "                            # Make sure all values are positive numbers for the size parameter\n",
        "                            plot_df['marker_size'] = plot_df['post_count'].clip(lower=1) * 20  # Scale for visibility\n",
        "\n",
        "                            # Create scatter plot with explicit marker size\n",
        "                            scatter = plt.scatter(\n",
        "                                x=plot_df['compound_score_mean'],\n",
        "                                y=plot_df['next_day_return'],\n",
        "                                alpha=0.7,\n",
        "                                c=plot_df['post_count'],  # This sets the color\n",
        "                                cmap='viridis',\n",
        "                                s=plot_df['marker_size']  # This sets the size and ensures it's always valid\n",
        "                            )\n",
        "                        else:\n",
        "                            # Use a fixed size if post_count is not available\n",
        "                            scatter = plt.scatter(\n",
        "                                x=plot_df['compound_score_mean'],\n",
        "                                y=plot_df['next_day_return'],\n",
        "                                alpha=0.7,\n",
        "                                color='blue',\n",
        "                                s=70  # Fixed size\n",
        "                            )\n",
        "\n",
        "                        plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
        "                        plt.axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
        "\n",
        "                        plt.xlabel('Sentiment Score', fontsize=12)\n",
        "                        plt.ylabel('Next Day Return (%)', fontsize=12)\n",
        "                        plt.title(f'{ticker_symbol} Sentiment vs. Next Day Returns', fontsize=14)\n",
        "                        plt.grid(True, alpha=0.3)\n",
        "\n",
        "                        # Add colorbar if using post count for coloring and we have at least 2 different values\n",
        "                        if 'post_count' in plot_df.columns and plot_df['post_count'].nunique() > 1:\n",
        "                            cbar = plt.colorbar(scatter)\n",
        "                            cbar.set_label('Number of Posts')\n",
        "\n",
        "                        # Add regression line\n",
        "                        try:\n",
        "                            from sklearn.linear_model import LinearRegression\n",
        "\n",
        "                            # Prepare data for regression\n",
        "                            X = plot_df['compound_score_mean'].values.reshape(-1, 1)\n",
        "                            y = plot_df['next_day_return'].values\n",
        "\n",
        "                            # Train model\n",
        "                            model = LinearRegression()\n",
        "                            model.fit(X, y)\n",
        "\n",
        "                            # Calculate R-squared\n",
        "                            r_squared = model.score(X, y)\n",
        "\n",
        "                            # Get predictions for plotting\n",
        "                            x_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
        "                            y_line = model.predict(x_line)\n",
        "\n",
        "                            plt.plot(\n",
        "                                x_line.flatten(),\n",
        "                                y_line,\n",
        "                                'r-',\n",
        "                                label=f'Slope: {model.coef_[0]:.4f}, R²: {r_squared:.4f}'\n",
        "                            )\n",
        "                            plt.legend(fontsize=10)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error creating regression line: {e}\")\n",
        "\n",
        "                        plt.savefig(os.path.join(self.viz_dir, f\"{ticker_symbol}_sentiment_vs_returns.png\"))\n",
        "                    else:\n",
        "                        print(f\"Not enough valid data points to create sentiment vs returns scatter plot for {ticker_symbol}\")\n",
        "\n",
        "                    plt.close(fig)  # Explicitly close the figure\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating sentiment vs returns plot: {e}\")\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "\n",
        "            # ===== Plot 4: Enhanced distribution of sentiment scores =====\n",
        "            if 'compound_score_mean' in merged_df.columns:\n",
        "                try:\n",
        "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "                    # Create histogram of sentiment scores - no NaN values\n",
        "                    sentiment_values = merged_df['compound_score_mean'].dropna()\n",
        "                    if len(sentiment_values) > 0:\n",
        "                        ax1.hist(sentiment_values, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
        "                        ax1.axvline(x=0, color='r', linestyle='--')\n",
        "                        mean_sentiment = sentiment_values.mean()\n",
        "                        ax1.axvline(x=mean_sentiment, color='blue', linestyle='-',\n",
        "                                  label=f'Mean: {mean_sentiment:.3f}')\n",
        "                        ax1.set_xlabel('Sentiment Score')\n",
        "                        ax1.set_ylabel('Frequency')\n",
        "                        ax1.set_title('Distribution of Reddit Sentiment Scores')\n",
        "                        ax1.legend()\n",
        "                        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Create sentiment categories and boxplot - no NaN values\n",
        "                    cat_data = merged_df.dropna(subset=['compound_score_mean', 'daily_return'])\n",
        "\n",
        "                    if len(cat_data) > 0:\n",
        "                        # Create sentiment categories\n",
        "                        cat_data['sentiment_category'] = pd.cut(\n",
        "                            cat_data['compound_score_mean'],\n",
        "                            bins=[-1, -0.5, -0.2, 0.2, 0.5, 1],\n",
        "                            labels=['Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive']\n",
        "                        )\n",
        "\n",
        "                        # Plot returns by sentiment category if we have enough data in each category\n",
        "                        category_counts = cat_data['sentiment_category'].value_counts()\n",
        "                        if (category_counts > 2).any():  # At least one category has more than 2 data points\n",
        "                            sns.boxplot(x='sentiment_category', y='daily_return', data=cat_data, ax=ax2)\n",
        "                            ax2.set_xlabel('Sentiment Category')\n",
        "                            ax2.set_ylabel('Daily Return (%)')\n",
        "                            ax2.set_title('Return Distribution by Sentiment Category')\n",
        "                            ax2.grid(True, alpha=0.3)\n",
        "                            plt.setp(ax2.get_xticklabels(), rotation=45)\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(os.path.join(self.viz_dir, f\"{ticker_symbol}_sentiment_analysis.png\"))\n",
        "                    plt.close(fig)  # Explicitly close the figure\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating sentiment distribution plot: {e}\")\n",
        "\n",
        "            # ===== Plot 5: Technical indicators visualization =====\n",
        "            if 'rsi_14' in merged_df.columns and 'macd' in merged_df.columns:\n",
        "                try:\n",
        "                    # Check if we have enough valid data\n",
        "                    if merged_df['rsi_14'].notna().sum() > 5 and merged_df['macd'].notna().sum() > 5:\n",
        "                        fig = plt.figure(figsize=(14, 10))\n",
        "\n",
        "                        # Create 4 vertically stacked subplots\n",
        "                        gs = fig.add_gridspec(4, 1, height_ratios=[3, 1, 1, 1], hspace=0.1)\n",
        "\n",
        "                        # Price subplot\n",
        "                        ax1 = fig.add_subplot(gs[0])\n",
        "                        ax1.set_title(f'{ticker_symbol} Technical Indicators', fontsize=14)\n",
        "                        ax1.plot(merged_df['Date'], merged_df['Close'], color='black', linewidth=2, label='Close Price')\n",
        "\n",
        "                        # Add moving averages\n",
        "                        if 'sma_20' in merged_df.columns:\n",
        "                            ax1.plot(merged_df['Date'], merged_df['sma_20'], color='blue', linewidth=1.5, label='SMA 20')\n",
        "                        if 'ema_10' in merged_df.columns:\n",
        "                            ax1.plot(merged_df['Date'], merged_df['ema_10'], color='purple', linewidth=1.5, label='EMA 10')\n",
        "\n",
        "                        # Add Bollinger Bands\n",
        "                        if ('bb_upper' in merged_df.columns and\n",
        "                            merged_df['bb_upper'].notna().all() and\n",
        "                            merged_df['bb_lower'].notna().all()):\n",
        "                            ax1.plot(merged_df['Date'], merged_df['bb_upper'], 'r--', linewidth=1, label='Bollinger Upper')\n",
        "                            ax1.plot(merged_df['Date'], merged_df['bb_lower'], 'r--', linewidth=1, label='Bollinger Lower')\n",
        "                            ax1.fill_between(merged_df['Date'], merged_df['bb_upper'], merged_df['bb_lower'],\n",
        "                                          color='gray', alpha=0.1)\n",
        "\n",
        "                        ax1.set_ylabel('Price ($)', fontsize=12)\n",
        "                        ax1.grid(True, alpha=0.3)\n",
        "                        ax1.legend(loc='upper left')\n",
        "\n",
        "                        # Volume subplot\n",
        "                        ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
        "                        ax2.bar(merged_df['Date'], merged_df['Volume'], color='gray', alpha=0.5)\n",
        "                        ax2.set_ylabel('Volume', fontsize=12)\n",
        "                        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "                        # RSI subplot\n",
        "                        ax3 = fig.add_subplot(gs[2], sharex=ax1)\n",
        "                        ax3.plot(merged_df['Date'], merged_df['rsi_14'], color='green', linewidth=1.5)\n",
        "                        ax3.axhline(y=70, color='r', linestyle='--', alpha=0.5)\n",
        "                        ax3.axhline(y=30, color='g', linestyle='--', alpha=0.5)\n",
        "                        ax3.set_ylabel('RSI (14)', fontsize=12)\n",
        "                        ax3.set_ylim(0, 100)\n",
        "                        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "                        # MACD subplot\n",
        "                        ax4 = fig.add_subplot(gs[3], sharex=ax1)\n",
        "                        ax4.plot(merged_df['Date'], merged_df['macd'], color='blue', linewidth=1.5, label='MACD')\n",
        "                        ax4.plot(merged_df['Date'], merged_df['macd_signal'], color='red', linewidth=1.5, label='Signal')\n",
        "\n",
        "                        # Add MACD histogram - safely handle possible NaN values\n",
        "                        # Loop through each row and create histogram bars\n",
        "                        for i in range(len(merged_df) - 1):\n",
        "                            if (pd.notna(merged_df['macd'].iloc[i]) and\n",
        "                                pd.notna(merged_df['macd_signal'].iloc[i])):\n",
        "\n",
        "                                # Determine color based on MACD vs Signal\n",
        "                                color = 'green' if merged_df['macd'].iloc[i] > merged_df['macd_signal'].iloc[i] else 'red'\n",
        "\n",
        "                                # Calculate histogram value\n",
        "                                hist_val = merged_df['macd'].iloc[i] - merged_df['macd_signal'].iloc[i]\n",
        "\n",
        "                                # Plot histogram bar\n",
        "                                ax4.bar(\n",
        "                                    merged_df['Date'].iloc[i],\n",
        "                                    hist_val,\n",
        "                                    color=color,\n",
        "                                    alpha=0.5,\n",
        "                                    width=1\n",
        "                                )\n",
        "\n",
        "                        ax4.set_ylabel('MACD', fontsize=12)\n",
        "                        ax4.grid(True, alpha=0.3)\n",
        "                        ax4.legend(loc='upper left')\n",
        "\n",
        "                        # Set x-axis label only for bottom subplot\n",
        "                        ax4.set_xlabel('Date', fontsize=12)\n",
        "\n",
        "                        # Remove x-axis labels for upper subplots\n",
        "                        ax1.set_xticklabels([])\n",
        "                        ax2.set_xticklabels([])\n",
        "                        ax3.set_xticklabels([])\n",
        "\n",
        "                        # Save figure without using tight_layout (which causes the warning)\n",
        "                        plt.subplots_adjust(hspace=0.3)\n",
        "                        plt.savefig(os.path.join(self.viz_dir, f\"{ticker_symbol}_technical_indicators.png\"))\n",
        "                        plt.close(fig)  # Explicitly close the figure\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating technical indicators plot: {e}\")\n",
        "\n",
        "            # Save statistics to JSON\n",
        "            try:\n",
        "                with open(stats_file, 'w') as f:\n",
        "                    json.dump(stats, f, indent=2)\n",
        "\n",
        "                print(f\"✅ Analysis visualizations saved to {self.viz_dir}\")\n",
        "                print(f\"✅ Statistics saved to {stats_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving statistics to file: {e}\")\n",
        "\n",
        "            return stats\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in analyze_data for {ticker_symbol}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()  # Print the full traceback for debugging\n",
        "            return None\n",
        "            \"\"\"\n",
        "            Analyze the merged data and create visualizations\n",
        "\n",
        "            Parameters:\n",
        "            ticker_symbol (str): Stock ticker symbol\n",
        "\n",
        "            Returns:\n",
        "            dict: Analysis statistics\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # Load merged data\n",
        "                merged_file = os.path.join(self.processed_dir, f\"{ticker_symbol}_merged.csv\")\n",
        "                if not os.path.exists(merged_file):\n",
        "                    print(f\"Merged data file not found: {merged_file}\")\n",
        "                    return None\n",
        "\n",
        "                merged_df = pd.read_csv(merged_file)\n",
        "\n",
        "                # Handle datetime conversion safely\n",
        "                try:\n",
        "                    merged_df['Date'] = pd.to_datetime(merged_df['Date'], utc=True)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error converting dates in analysis for {ticker_symbol}: {e}\")\n",
        "                    return None\n",
        "\n",
        "                print(f\"Analyzing data for {ticker_symbol}...\")\n",
        "\n",
        "                # Plot 1: Enhanced stock price and sentiment over time with volume\n",
        "                if 'compound_score_mean' in merged_df.columns:\n",
        "                    fig = plt.figure(figsize=(14, 10))\n",
        "\n",
        "                    # Create 3 vertically stacked subplots\n",
        "                    gs = fig.add_gridspec(3, 1, height_ratios=[3, 1, 1], hspace=0.1)\n",
        "\n",
        "                    # Stock price subplot\n",
        "                    ax1 = fig.add_subplot(gs[0])\n",
        "                    ax1.set_title(f'{ticker_symbol} Stock Price vs. Reddit Sentiment', fontsize=14)\n",
        "                    ax1.plot(merged_df['Date'], merged_df['Close'], color='blue', linewidth=2, label='Close Price')\n",
        "                    ax1.set_ylabel('Stock Price ($)', color='blue', fontsize=12)\n",
        "                    ax1.tick_params(axis='y', labelcolor='blue')\n",
        "                    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Add sentiment line on secondary y-axis\n",
        "                    ax2 = ax1.twinx()\n",
        "                    ax2.plot(merged_df['Date'], merged_df['compound_score_mean'], color='green',\n",
        "                          linestyle='--', linewidth=2, label='Sentiment Score')\n",
        "                    ax2.set_ylabel('Sentiment Score', color='green', fontsize=12)\n",
        "                    ax2.tick_params(axis='y', labelcolor='green')\n",
        "\n",
        "                    # Create legend with both price and sentiment\n",
        "                    lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "                    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "                    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "                    # Volume subplot\n",
        "                    ax3 = fig.add_subplot(gs[1], sharex=ax1)\n",
        "                    ax3.bar(merged_df['Date'], merged_df['Volume'], color='gray', alpha=0.6, label='Volume')\n",
        "                    ax3.set_ylabel('Volume', color='gray', fontsize=12)\n",
        "                    ax3.tick_params(axis='y', labelcolor='gray')\n",
        "                    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Sentiment and post count subplot\n",
        "                    ax4 = fig.add_subplot(gs[2], sharex=ax1)\n",
        "                    if 'post_count' in merged_df.columns:\n",
        "                        ax4.bar(merged_df['Date'], merged_df['post_count'], color='orange', alpha=0.6, label='Post Count')\n",
        "                        ax4.set_ylabel('Post Count', color='orange', fontsize=12)\n",
        "                        ax4.tick_params(axis='y', labelcolor='orange')\n",
        "                        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Remove x-axis labels for upper subplots\n",
        "                    ax1.set_xticklabels([])\n",
        "                    ax3.set_xticklabels([])\n",
        "                    ax4.set_xlabel('Date', fontsize=12)\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(os.path.join(self.viz_dir, f\"{ticker_symbol}_price_vs_sentiment.png\"))\n",
        "                    plt.close()\n",
        "\n",
        "                    # Also create interactive Plotly version\n",
        "                    fig = go.Figure()\n",
        "\n",
        "                    # Add price candlestick\n",
        "                    fig.add_trace(\n",
        "                        go.Candlestick(\n",
        "                            x=merged_df['Date'],\n",
        "                            open=merged_df['Open'],\n",
        "                            high=merged_df['High'],\n",
        "                            low=merged_df['Low'],\n",
        "                            close=merged_df['Close'],\n",
        "                            name='OHLC',\n",
        "                            increasing_line_color='green',\n",
        "                            decreasing_line_color='red'\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    # Add moving averages if available\n",
        "                    if 'sma_20' in merged_df.columns:\n",
        "                      fig.add_trace(\n",
        "                        go.Scatter(\n",
        "                            x=merged_df['Date'],\n",
        "                            y=merged_df['sma_20'],\n",
        "                            name='20-day MA',\n",
        "                            line=dict(color='purple', width=1)\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                    # Add sentiment overlay\n",
        "                    fig.add_trace(\n",
        "                        go.Scatter(\n",
        "                            x=merged_df['Date'],\n",
        "                            y=merged_df['compound_score_mean'],\n",
        "                            name='Sentiment Score',\n",
        "                            line=dict(color='green', width=2, dash='dash'),\n",
        "                            yaxis='y2'\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    # Add RSI if available\n",
        "                    if 'rsi_14' in merged_df.columns:\n",
        "                        fig.add_trace(\n",
        "                            go.Scatter(\n",
        "                                x=merged_df['Date'],\n",
        "                                y=merged_df['rsi_14'],\n",
        "                                name='RSI (14)',\n",
        "                                line=dict(color='orange', width=1),\n",
        "                                visible='legendonly',  # Hidden by default, can be toggled\n",
        "                                yaxis='y3'\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                    # Configure layout with multiple y-axes\n",
        "                    fig.update_layout(\n",
        "                        title=f'{ticker_symbol} Stock Price vs. Reddit Sentiment',\n",
        "                        xaxis_title='Date',\n",
        "                        yaxis_title='Stock Price ($)',\n",
        "                        yaxis2=dict(\n",
        "                            title='Sentiment Score',\n",
        "                            titlefont=dict(color='green'),\n",
        "                            tickfont=dict(color='green'),\n",
        "                            overlaying='y',\n",
        "                            side='right',\n",
        "                            range=[-1, 1]\n",
        "                        ),\n",
        "                        yaxis3=dict(\n",
        "                            title='RSI',\n",
        "                            titlefont=dict(color='orange'),\n",
        "                            tickfont=dict(color='orange'),\n",
        "                            anchor='free',\n",
        "                            overlaying='y',\n",
        "                            side='right',\n",
        "                            position=0.95,\n",
        "                            range=[0, 100],\n",
        "                            showgrid=False\n",
        "                        ),\n",
        "                        legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.8)'),\n",
        "                        template='plotly_white',\n",
        "                        margin=dict(l=50, r=70, t=50, b=50),\n",
        "                        height=700\n",
        "                    )\n",
        "\n",
        "                    fig.write_html(os.path.join(self.viz_dir, f\"{ticker_symbol}_interactive_analysis.html\"))\n",
        "\n",
        "                # Plot 2: Trading volume and social media activity\n",
        "                if 'post_count' in merged_df.columns:\n",
        "                    fig = plt.figure(figsize=(14, 6))\n",
        "\n",
        "                    ax1 = plt.gca()\n",
        "                    ax1.set_xlabel('Date')\n",
        "                    ax1.set_ylabel('Trading Volume', color='blue')\n",
        "                    ax1.bar(merged_df['Date'], merged_df['Volume'], color='blue', alpha=0.5)\n",
        "                    ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "                    ax2 = ax1.twinx()\n",
        "                    ax2.set_ylabel('Number of Reddit Posts', color='red')\n",
        "                    ax2.plot(merged_df['Date'], merged_df['post_count'], color='red', marker='o', linestyle='-')\n",
        "                    ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "                    plt.title(f'{ticker_symbol} Trading Volume vs. Reddit Activity')\n",
        "                    plt.grid(True, alpha=0.3)\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(os.path.join(self.viz_dir, f\"{ticker_symbol}_volume_vs_activity.png\"))\n",
        "                    plt.close()\n",
        "\n",
        "                # Plot 3: Enhanced correlation between sentiment and next day returns\n",
        "                if 'compound_score_mean' in merged_df.columns:\n",
        "                    fig = plt.figure(figsize=(10, 8))\n",
        "\n",
        "                    # Remove NaN values\n",
        "                    plot_df = merged_df.dropna(subset=['daily_return', 'compound_score_mean'])\n",
        "\n",
        "\n",
        "                    # Create color based on upvote count if available\n",
        "                    color_values = plot_df['post_count'] if 'post_count' in plot_df.columns else 'blue'\n",
        "\n",
        "                    # Create scatter plot\n",
        "                    scatter = plt.scatter(plot_df['compound_score_mean'],\n",
        "                                        plot_df['daily_return'].shift(-1),\n",
        "                                        alpha=0.7,\n",
        "                                        c=color_values,\n",
        "                                        cmap='viridis',\n",
        "                                        s=70)  # Increased point size\n",
        "\n",
        "                    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
        "                    plt.axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
        "\n",
        "                    plt.xlabel('Sentiment Score', fontsize=12)\n",
        "                    plt.ylabel('Next Day Return (%)', fontsize=12)\n",
        "                    plt.title(f'{ticker_symbol} Sentiment vs. Next Day Returns', fontsize=14)\n",
        "                    plt.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Add colorbar if using post count for coloring\n",
        "                    if 'post_count' in plot_df.columns:\n",
        "                        cbar = plt.colorbar(scatter)\n",
        "                        cbar.set_label('Number of Posts')\n",
        "\n",
        "                    # Add regression line\n",
        "                    if len(plot_df) > 2:  # Need at least 3 points for regression\n",
        "                        from scipy import stats\n",
        "                        from sklearn.linear_model import LinearRegression\n",
        "\n",
        "                        # Simple linear regression\n",
        "                        mask = ~np.isnan(plot_df['daily_return'].shift(-1))\n",
        "                        if sum(mask) > 2:\n",
        "                            # Scikit-learn for more robust calculation\n",
        "                            X = plot_df.loc[mask, 'compound_score_mean'].values.reshape(-1, 1)\n",
        "                            y = plot_df.loc[mask, 'daily_return'].shift(-1).values\n",
        "\n",
        "                            model = LinearRegression()\n",
        "                            model.fit(X, y)\n",
        "\n",
        "                            # Calculate R-squared\n",
        "                            r_squared = model.score(X, y)\n",
        "\n",
        "                            # Get predictions for plotting\n",
        "                            x_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
        "                            y_line = model.predict(x_line)\n",
        "\n",
        "                            plt.plot(\n",
        "                                x_line.flatten(),\n",
        "                                y_line,\n",
        "                                'r-',\n",
        "                                label=f'Slope: {model.coef_[0]:.4f}, R²: {r_squared:.4f}'\n",
        "                            )\n",
        "                            plt.legend(fontsize=10)\n",
        "\n",
        "                    # Add annotations for extreme values\n",
        "                    if len(plot_df) > 0:\n",
        "                        # Find the top 3 and bottom 3 returns\n",
        "                        top_returns = plot_df.nlargest(3, 'daily_return')\n",
        "                        bottom_returns = plot_df.nsmallest(3, 'daily_return')\n",
        "\n",
        "                        # Annotate these points\n",
        "                        for _, row in pd.concat([top_returns, bottom_returns]).iterrows():\n",
        "                            plt.annotate(\n",
        "                                f\"{row['daily_return']:.2f}%\",\n",
        "                                xy=(row['compound_score_mean'], row['daily_return']),\n",
        "                                xytext=(5, 5),\n",
        "                                textcoords='offset points',\n",
        "                                fontsize=8,\n",
        "                                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.7)\n",
        "                            )\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(os.path.join(self.viz_dir, f\"{ticker_symbol}_sentiment_vs_returns.png\"))\n",
        "                    plt.close()\n",
        "\n",
        "                # Plot 4: Enhanced distribution of sentiment scores\n",
        "                if 'compound_score_mean' in merged_df.columns:\n",
        "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "                    # Create histogram of sentiment scores\n",
        "                    ax1.hist(merged_df['compound_score_mean'], bins=20, alpha=0.7, color='green', edgecolor='black')\n",
        "                    ax1.axvline(x=0, color='r', linestyle='--')\n",
        "                    ax1.axvline(x=merged_df['compound_score_mean'].mean(), color='blue', linestyle='-',\n",
        "                              label=f'Mean: {merged_df[\"compound_score_mean\"].mean():.3f}')\n",
        "                    ax1.set_xlabel('Sentiment Score')\n",
        "                    ax1.set_ylabel('Frequency')\n",
        "                    ax1.set_title(f'Distribution of Reddit Sentiment Scores')\n",
        "                    ax1.legend()\n",
        "                    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Create distribution of returns by sentiment category\n",
        "                    # Create sentiment categories\n",
        "                    merged_df['sentiment_category'] = pd.cut(\n",
        "                        merged_df['compound_score_mean'],\n",
        "                        bins=[-1, -0.5, -0.2, 0.2, 0.5, 1],\n",
        "                        labels=['Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive']\n",
        "                    )\n",
        "\n",
        "                    # Plot returns by sentiment category\n",
        "                    sns.boxplot(x='sentiment_category', y='daily_return', data=merged_df, ax=ax2)\n",
        "                    ax2.set_xlabel('Sentiment Category')\n",
        "                    ax2.set_ylabel('Daily Return (%)')\n",
        "                    ax2.set_title('Return Distribution by Sentiment Category')\n",
        "                    ax2.grid(True, alpha=0.3)\n",
        "                    plt.xticks(rotation=45)\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(os.path.join(self.viz_dir, f\"{ticker_symbol}_sentiment_analysis.png\"))\n",
        "                    plt.close()\n",
        "\n",
        "                # Plot 5: Technical indicators visualization\n",
        "                if 'rsi_14' in merged_df.columns and 'macd' in merged_df.columns:\n",
        "                    fig = plt.figure(figsize=(14, 10))\n",
        "\n",
        "                    # Create 4 vertically stacked subplots\n",
        "                    gs = fig.add_gridspec(4, 1, height_ratios=[3, 1, 1, 1], hspace=0.1)\n",
        "\n",
        "                    # Price subplot\n",
        "                    ax1 = fig.add_subplot(gs[0])\n",
        "                    ax1.set_title(f'{ticker_symbol} Technical Indicators', fontsize=14)\n",
        "                    ax1.plot(merged_df['Date'], merged_df['Close'], color='black', linewidth=2, label='Close Price')\n",
        "\n",
        "                    # Add moving averages\n",
        "                    if 'sma_20' in merged_df.columns:\n",
        "                        ax1.plot(merged_df['Date'], merged_df['sma_20'], color='blue', linewidth=1.5, label='SMA 20')\n",
        "                    if 'ema_10' in merged_df.columns:\n",
        "                        ax1.plot(merged_df['Date'], merged_df['ema_10'], color='purple', linewidth=1.5, label='EMA 10')\n",
        "\n",
        "                    # Add Bollinger Bands\n",
        "                    if 'bb_upper' in merged_df.columns:\n",
        "                        ax1.plot(merged_df['Date'], merged_df['bb_upper'], 'r--', linewidth=1, label='Bollinger Upper')\n",
        "                        ax1.plot(merged_df['Date'], merged_df['bb_lower'], 'r--', linewidth=1, label='Bollinger Lower')\n",
        "                        ax1.fill_between(merged_df['Date'], merged_df['bb_upper'], merged_df['bb_lower'], color='gray', alpha=0.1)\n",
        "\n",
        "                    ax1.set_ylabel('Price ($)', fontsize=12)\n",
        "                    ax1.grid(True, alpha=0.3)\n",
        "                    ax1.legend(loc='upper left')\n",
        "\n",
        "                    # Volume subplot\n",
        "                    ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
        "                    ax2.bar(merged_df['Date'], merged_df['Volume'], color='gray', alpha=0.5)\n",
        "                    ax2.set_ylabel('Volume', fontsize=12)\n",
        "                    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "                    # RSI subplot\n",
        "                    ax3 = fig.add_subplot(gs[2], sharex=ax1)\n",
        "                    ax3.plot(merged_df['Date'], merged_df['rsi_14'], color='green', linewidth=1.5)\n",
        "                    ax3.axhline(y=70, color='r', linestyle='--', alpha=0.5)\n",
        "                    ax3.axhline(y=30, color='g', linestyle='--', alpha=0.5)\n",
        "                    ax3.set_ylabel('RSI (14)', fontsize=12)\n",
        "                    ax3.set_ylim(0, 100)\n",
        "                    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "                    # MACD subplot\n",
        "                    ax4 = fig.add_subplot(gs[3], sharex=ax1)\n",
        "                    ax4.plot(merged_df['Date'], merged_df['macd'], color='blue', linewidth=1.5, label='MACD')\n",
        "                    ax4.plot(merged_df['Date'], merged_df['macd_signal'], color='red', linewidth=1.5, label='Signal')\n",
        "\n",
        "                    # Add MACD histogram\n",
        "                    for i in range(len(merged_df) - 1):\n",
        "                        if merged_df['macd'].iloc[i] > merged_df['macd_signal'].iloc[i]:\n",
        "                            color = 'green'\n",
        "                        else:\n",
        "                            color = 'red'\n",
        "                        ax4.bar(merged_df['Date'].iloc[i], merged_df['macd'].iloc[i] - merged_df['macd_signal'].iloc[i],\n",
        "                              color=color, alpha=0.5, width=1)\n",
        "\n",
        "                    ax4.set_ylabel('MACD', fontsize=12)\n",
        "                    ax4.grid(True, alpha=0.3)\n",
        "                    ax4.legend(loc='upper left')\n",
        "\n",
        "                    # Set x-axis label only for bottom subplot\n",
        "                    ax4.set_xlabel('Date', fontsize=12)\n",
        "\n",
        "                    # Remove x-axis labels for upper subplots\n",
        "                    ax1.set_xticklabels([])\n",
        "                    ax2.set_xticklabels([])\n",
        "                    ax3.set_xticklabels([])\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(os.path.join(self.viz_dir, f\"{ticker_symbol}_technical_indicators.png\"))\n",
        "                    plt.close()\n",
        "\n",
        "                # Statistical analysis\n",
        "                stats_file = os.path.join(self.processed_dir, f\"{ticker_symbol}_stats.json\")\n",
        "                stats = {\n",
        "                    'ticker': ticker_symbol,\n",
        "                    'data_points': len(merged_df),\n",
        "                    'date_range': [merged_df['Date'].min().strftime('%Y-%m-%d'),\n",
        "                                  merged_df['Date'].max().strftime('%Y-%m-%d')],\n",
        "                    'avg_close': float(merged_df['Close'].mean()),\n",
        "                    'min_close': float(merged_df['Close'].min()),\n",
        "                    'max_close': float(merged_df['Close'].max()),\n",
        "                    'stddev_close': float(merged_df['Close'].std()),\n",
        "                    'avg_volume': float(merged_df['Volume'].mean()),\n",
        "                    'avg_daily_return': float(merged_df['daily_return'].mean()),\n",
        "                    'stddev_daily_return': float(merged_df['daily_return'].std()),\n",
        "                    'up_days_pct': float((merged_df['daily_return'] > 0).mean() * 100)\n",
        "                }\n",
        "\n",
        "                # Add technical indicator statistics if available\n",
        "                if 'rsi_14' in merged_df.columns:\n",
        "                    stats.update({\n",
        "                        'avg_rsi': float(merged_df['rsi_14'].mean()),\n",
        "                        'overbought_days_pct': float((merged_df['rsi_14'] > 70).mean() * 100),\n",
        "                        'oversold_days_pct': float((merged_df['rsi_14'] < 30).mean() * 100)\n",
        "                    })\n",
        "\n",
        "                if 'volatility_daily' in merged_df.columns:\n",
        "                    stats.update({\n",
        "                        'avg_volatility': float(merged_df['volatility_daily'].mean()),\n",
        "                        'max_volatility': float(merged_df['volatility_daily'].max())\n",
        "                    })\n",
        "\n",
        "                if 'compound_score_mean' in merged_df.columns:\n",
        "                    # Add sentiment statistics\n",
        "                    stats.update({\n",
        "                        'avg_sentiment': float(merged_df['compound_score_mean'].mean()),\n",
        "                        'min_sentiment': float(merged_df['compound_score_mean'].min()),\n",
        "                        'max_sentiment': float(merged_df['compound_score_mean'].max()),\n",
        "                        'stddev_sentiment': float(merged_df['compound_score_mean'].std()),\n",
        "                        'positive_days_pct': float((merged_df['compound_score_mean'] > 0).mean() * 100),\n",
        "                        'avg_posts_per_day': float(merged_df['post_count'].mean() if 'post_count' in merged_df.columns else 0),\n",
        "                        'sentiment_return_corr': float(merged_df['compound_score_mean'].corr(merged_df['daily_return'].shift(-1)))\n",
        "                    })\n",
        "\n",
        "                    # Add engagement statistics if available\n",
        "                    if 'engagement_ratio' in merged_df.columns:\n",
        "                        stats.update({\n",
        "                            'avg_engagement': float(merged_df['engagement_ratio'].mean()),\n",
        "                            'max_engagement': float(merged_df['engagement_ratio'].max())\n",
        "                        })\n",
        "\n",
        "                # Save statistics\n",
        "                with open(stats_file, 'w') as f:\n",
        "                    json.dump(stats, f, indent=2)\n",
        "\n",
        "                print(f\"✅ Analysis visualizations saved to {self.viz_dir}\")\n",
        "                print(f\"✅ Statistics saved to {stats_file}\")\n",
        "\n",
        "                return stats\n",
        "\n",
        "            except Exception as e:\n",
        "                  print(f\"Error in analyze_data for {ticker_symbol}: {e}\")\n",
        "                  return None\n",
        "\n",
        "    def train_models(self, ticker_symbols=None):\n",
        "        \"\"\"\n",
        "        Train both linear and logistic regression models with improved methodology\n",
        "\n",
        "        Parameters:\n",
        "        ticker_symbols (list): List of ticker symbols to use for training\n",
        "\n",
        "        Returns:\n",
        "        tuple: (linear_model, logistic_model, scaler, features)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # If no tickers specified, use all available merged data files\n",
        "            if ticker_symbols is None:\n",
        "                merged_files = [f for f in os.listdir(self.processed_dir) if f.endswith('_merged.csv')]\n",
        "                ticker_symbols = [f.split('_')[0] for f in merged_files]\n",
        "\n",
        "            if not ticker_symbols:\n",
        "                print(\"No data files found for model training\")\n",
        "                return None, None, None, None\n",
        "\n",
        "            print(f\"Training models using data from: {ticker_symbols}\")\n",
        "\n",
        "            # Combine data from all tickers\n",
        "            all_data = []\n",
        "            for ticker in ticker_symbols:\n",
        "                try:\n",
        "                    merged_file = os.path.join(self.processed_dir, f\"{ticker}_merged.csv\")\n",
        "                    if os.path.exists(merged_file):\n",
        "                        ticker_df = pd.read_csv(merged_file)\n",
        "\n",
        "                        # Ensure we have the required target columns\n",
        "                        if 'daily_return' not in ticker_df.columns or 'price_up_next_day' not in ticker_df.columns:\n",
        "                            print(f\"Skipping {ticker} - missing required target columns\")\n",
        "                            continue\n",
        "\n",
        "                        # Convert Date to datetime\n",
        "                        if 'Date' in ticker_df.columns:\n",
        "                            ticker_df['Date'] = pd.to_datetime(ticker_df['Date'])\n",
        "\n",
        "                        ticker_df['ticker'] = ticker\n",
        "                        all_data.append(ticker_df)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading data for {ticker}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if not all_data:\n",
        "                print(\"No data available for model training\")\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Combine all data\n",
        "            combined_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "            # Handle any string 'Date' columns that might cause isfinite errors\n",
        "            if 'Date' in combined_df.columns:\n",
        "                combined_df = combined_df.drop(columns=['Date'])\n",
        "\n",
        "            if 'date_only' in combined_df.columns:\n",
        "                combined_df = combined_df.drop(columns=['date_only'])\n",
        "\n",
        "            # Define potential features based on what's available\n",
        "            # Base features (price data)\n",
        "            base_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "            # Technical indicator features\n",
        "            tech_features = [col for col in combined_df.columns if col in [\n",
        "                'sma_5', 'sma_10', 'sma_20', 'ema_5', 'ema_10', 'ema_20',\n",
        "                'rsi_14', 'macd', 'macd_diff', 'bb_width', 'stoch_k',\n",
        "                'volatility_daily', 'price_to_sma20', 'gap', 'atr',\n",
        "                'ema_5_10_cross', 'ema_10_20_cross', 'roc_5', 'close_pct_change'\n",
        "            ]]\n",
        "\n",
        "            # Sentiment features\n",
        "            sentiment_features = [col for col in combined_df.columns if col in [\n",
        "                'compound_score_mean', 'positive_score_mean', 'negative_score_mean',\n",
        "                'sentiment_bias', 'sentiment_dispersion', 'post_count',\n",
        "                'upvotes_sum', 'num_comments_sum', 'engagement_ratio',\n",
        "                'sent_momentum', 'sent_roll_mean_3', 'sent_price_interaction'\n",
        "            ]]\n",
        "\n",
        "            # Combine all available features\n",
        "            available_features = base_features + tech_features + sentiment_features\n",
        "\n",
        "            # Remove any columns that aren't numeric\n",
        "            numeric_cols = combined_df.select_dtypes(include=['number']).columns\n",
        "            available_features = [col for col in available_features if col in numeric_cols]\n",
        "\n",
        "            # Remove target variables from features\n",
        "            available_features = [col for col in available_features if col not in ['daily_return', 'price_up_next_day']]\n",
        "\n",
        "            # Check if we have enough features\n",
        "            if len(available_features) <= 5:\n",
        "                print(\"Not enough features available for model training. Using base features only.\")\n",
        "                available_features = [col for col in base_features if col in numeric_cols]\n",
        "\n",
        "            # Ensure all required columns exist\n",
        "            for feature in available_features:\n",
        "                if feature not in combined_df.columns:\n",
        "                    print(f\"Warning: {feature} not found in data, adding with zeros\")\n",
        "                    combined_df[feature] = 0\n",
        "\n",
        "            # Handle missing values and infinities\n",
        "            combined_df = combined_df.replace([np.inf, -np.inf], np.nan)\n",
        "            combined_df[available_features] = combined_df[available_features].fillna(0)\n",
        "\n",
        "            # Create additional features\n",
        "            if all(col in combined_df.columns for col in ['High', 'Low']):\n",
        "                combined_df['price_range'] = combined_df['High'] - combined_df['Low']\n",
        "                available_features.append('price_range')\n",
        "\n",
        "            if 'Volume' in combined_df.columns:\n",
        "                combined_df['volume_change'] = combined_df['Volume'].pct_change()\n",
        "                combined_df['volume_change'] = combined_df['volume_change'].fillna(0)\n",
        "                available_features.append('volume_change')\n",
        "\n",
        "            if all(col in combined_df.columns for col in ['Close', 'Open']):\n",
        "                combined_df['close_to_open'] = (combined_df['Close'] - combined_df['Open']) / combined_df['Open'] * 100\n",
        "                combined_df['close_to_open'] = combined_df['close_to_open'].fillna(0)\n",
        "                available_features.append('close_to_open')\n",
        "\n",
        "            # Create lag features for each ticker separately\n",
        "            for ticker in combined_df['ticker'].unique():\n",
        "                mask = combined_df['ticker'] == ticker\n",
        "                for feature in ['Close', 'Volume', 'daily_return']:\n",
        "                    if feature in combined_df.columns:\n",
        "                        lag_col1 = f'{feature}_lag1'\n",
        "                        lag_col2 = f'{feature}_lag2'\n",
        "\n",
        "                        combined_df.loc[mask, lag_col1] = combined_df.loc[mask, feature].shift(1)\n",
        "                        combined_df.loc[mask, lag_col2] = combined_df.loc[mask, feature].shift(2)\n",
        "\n",
        "                        combined_df[lag_col1] = combined_df[lag_col1].fillna(0)\n",
        "                        combined_df[lag_col2] = combined_df[lag_col2].fillna(0)\n",
        "\n",
        "                        available_features.extend([lag_col1, lag_col2])\n",
        "\n",
        "            # Add sentiment lag features if available\n",
        "            if 'compound_score_mean' in combined_df.columns:\n",
        "                for ticker in combined_df['ticker'].unique():\n",
        "                    mask = combined_df['ticker'] == ticker\n",
        "\n",
        "                    sent_lag1 = 'sentiment_lag1'\n",
        "                    sent_lag2 = 'sentiment_lag2'\n",
        "\n",
        "                    combined_df.loc[mask, sent_lag1] = combined_df.loc[mask, 'compound_score_mean'].shift(1)\n",
        "                    combined_df.loc[mask, sent_lag2] = combined_df.loc[mask, 'compound_score_mean'].shift(2)\n",
        "\n",
        "                    combined_df[sent_lag1] = combined_df[sent_lag1].fillna(0)\n",
        "                    combined_df[sent_lag2] = combined_df[sent_lag2].fillna(0)\n",
        "\n",
        "                    available_features.extend([sent_lag1, sent_lag2])\n",
        "\n",
        "            # Make feature list unique\n",
        "            available_features = list(set(available_features))\n",
        "\n",
        "            # Create dummy variables for ticker symbols\n",
        "            ticker_dummies = pd.get_dummies(combined_df['ticker'], prefix='ticker')\n",
        "            combined_df = pd.concat([combined_df, ticker_dummies], axis=1)\n",
        "            ticker_features = ticker_dummies.columns.tolist()\n",
        "            available_features.extend(ticker_features)\n",
        "\n",
        "            # Verify all features are numeric\n",
        "            numeric_features = []\n",
        "            for feature in available_features:\n",
        "                if feature in combined_df.columns:\n",
        "                    if np.issubdtype(combined_df[feature].dtype, np.number):\n",
        "                        numeric_features.append(feature)\n",
        "                    else:\n",
        "                        print(f\"Skipping non-numeric feature: {feature}\")\n",
        "\n",
        "            available_features = numeric_features\n",
        "\n",
        "            # Prepare features and targets\n",
        "            X = combined_df[available_features].values\n",
        "\n",
        "            # Ensure target variables are numeric and don't have NaN values\n",
        "            combined_df['daily_return'] = pd.to_numeric(combined_df['daily_return'], errors='coerce')\n",
        "            combined_df['price_up_next_day'] = pd.to_numeric(combined_df['price_up_next_day'], errors='coerce')\n",
        "\n",
        "            combined_df['daily_return'] = combined_df['daily_return'].fillna(0)\n",
        "            combined_df['price_up_next_day'] = combined_df['price_up_next_day'].fillna(0)\n",
        "            combined_df['price_up_next_day'] = combined_df['price_up_next_day'].astype(int)\n",
        "\n",
        "            y_linear = combined_df['daily_return'].values\n",
        "            y_logistic = combined_df['price_up_next_day'].values\n",
        "\n",
        "            # Check for NaN or infinite values in X and y\n",
        "            if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
        "                print(\"Warning: NaN or infinite values found in feature matrix X. Replacing with zeros.\")\n",
        "                X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            if np.any(np.isnan(y_linear)) or np.any(np.isinf(y_linear)):\n",
        "                print(\"Warning: NaN or infinite values found in y_linear. Replacing with zeros.\")\n",
        "                y_linear = np.nan_to_num(y_linear, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            if np.any(np.isnan(y_logistic)) or np.any(np.isinf(y_logistic)):\n",
        "                print(\"Warning: NaN or infinite values found in y_logistic. Replacing with zeros.\")\n",
        "                y_logistic = np.nan_to_num(y_logistic, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            # Scale features\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "            # Use time series split for validation\n",
        "            tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "            # Take the last split to have a separate test set\n",
        "            train_index = None\n",
        "            test_index = None\n",
        "\n",
        "            for train_idx, test_idx in tscv.split(X_scaled):\n",
        "                train_index = train_idx\n",
        "                test_index = test_idx\n",
        "\n",
        "            if train_index is None or test_index is None:\n",
        "                # Fall back to simple split if time series split fails\n",
        "                train_size = int(len(X_scaled) * 0.8)\n",
        "                train_index = np.arange(train_size)\n",
        "                test_index = np.arange(train_size, len(X_scaled))\n",
        "\n",
        "            X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "            y_linear_train, y_linear_test = y_linear[train_index], y_linear[test_index]\n",
        "            y_logistic_train, y_logistic_test = y_logistic[train_index], y_logistic[test_index]\n",
        "\n",
        "            print(f\"Training data shape: {X_train.shape}\")\n",
        "            print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n",
        "            # Use a simpler approach for feature selection to avoid errors\n",
        "            try:\n",
        "                print(\"Performing feature selection...\")\n",
        "                rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "                rf_selector.fit(X_train, y_logistic_train)\n",
        "\n",
        "                # Get feature importances\n",
        "                importances = rf_selector.feature_importances_\n",
        "\n",
        "                # Select top 70% features\n",
        "                num_features_to_keep = max(int(X_train.shape[1] * 0.7), 10)\n",
        "                indices = np.argsort(importances)[::-1][:num_features_to_keep]\n",
        "\n",
        "                # Print top features\n",
        "                print(\"\\nTop 10 features by importance:\")\n",
        "                for i in range(min(10, len(indices))):\n",
        "                    print(f\"{available_features[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
        "\n",
        "                # Create mask\n",
        "                mask = np.zeros(X_train.shape[1], dtype=bool)\n",
        "                mask[indices] = True\n",
        "\n",
        "                # Apply mask\n",
        "                X_train_selected = X_train[:, mask]\n",
        "                X_test_selected = X_test[:, mask]\n",
        "\n",
        "                # Get selected feature names\n",
        "                selected_feature_names = [available_features[i] for i in indices]\n",
        "\n",
        "                print(f\"Selected {X_train_selected.shape[1]} of {X_train.shape[1]} features\")\n",
        "            except Exception as e:\n",
        "                print(f\"Feature selection failed: {e}, using all features\")\n",
        "                X_train_selected = X_train\n",
        "                X_test_selected = X_test\n",
        "                mask = np.ones(X_train.shape[1], dtype=bool)\n",
        "                selected_feature_names = available_features\n",
        "\n",
        "            # Check for class imbalance\n",
        "            class_counts = np.bincount(y_logistic_train.astype(int))\n",
        "            class_balance = min(class_counts) / max(class_counts)\n",
        "            print(f\"Class distribution: {class_counts}, balance ratio: {class_balance:.2f}\")\n",
        "\n",
        "            # Apply SMOTE for class imbalance if needed and if we have enough samples\n",
        "            X_train_logistic = X_train_selected\n",
        "            if class_balance < 0.7 and min(class_counts) >= 5:\n",
        "                try:\n",
        "                    smote = SMOTE(random_state=42)\n",
        "                    X_train_logistic, y_logistic_train_resampled = smote.fit_resample(X_train_selected, y_logistic_train)\n",
        "                    print(f\"After SMOTE: {np.bincount(y_logistic_train_resampled.astype(int))}\")\n",
        "                    y_logistic_train = y_logistic_train_resampled\n",
        "                except Exception as e:\n",
        "                    print(f\"SMOTE failed: {e}, continuing with original data\")\n",
        "                    X_train_logistic = X_train_selected\n",
        "\n",
        "            # Train regression model for returns\n",
        "            print(\"\\nTraining regression model for stock returns...\")\n",
        "            try:\n",
        "                # Try Ridge regression\n",
        "                ridge = Ridge(alpha=0.1, random_state=42)\n",
        "                ridge.fit(X_train_selected, y_linear_train)\n",
        "                ridge_score = ridge.score(X_train_selected, y_linear_train)\n",
        "                print(f\"Ridge regression R² score: {ridge_score:.4f}\")\n",
        "\n",
        "                # Use Ridge as our final model\n",
        "                linear_model = ridge\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error training Ridge regression: {e}\")\n",
        "                # Fallback to simple linear regression\n",
        "                from sklearn.linear_model import LinearRegression\n",
        "                linear_model = LinearRegression()\n",
        "                linear_model.fit(X_train_selected, y_linear_train)\n",
        "                print(\"Fallback to simple LinearRegression\")\n",
        "\n",
        "            # Evaluate regression model\n",
        "            try:\n",
        "                linear_metrics = self._evaluate_linear_model(linear_model, X_test_selected, y_linear_test, selected_feature_names)\n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating regression model: {e}\")\n",
        "                linear_metrics = {\"error\": str(e)}\n",
        "\n",
        "            # Train classification model\n",
        "            print(\"\\nTraining classification model for price direction...\")\n",
        "            try:\n",
        "                # Use logistic regression which is more stable\n",
        "                logistic = LogisticRegression(C=1.0, class_weight='balanced',\n",
        "                                          penalty='l2', solver='liblinear',\n",
        "                                          random_state=42, max_iter=1000)\n",
        "                logistic.fit(X_train_logistic, y_logistic_train)\n",
        "                logistic_score = logistic.score(X_train_logistic, y_logistic_train)\n",
        "                print(f\"Logistic regression accuracy: {logistic_score:.4f}\")\n",
        "\n",
        "                # Use Logistic as our final model\n",
        "                logistic_model = logistic\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error training Logistic regression: {e}\")\n",
        "                # Fallback to a very simple model\n",
        "                from sklearn.dummy import DummyClassifier\n",
        "                logistic_model = DummyClassifier(strategy='most_frequent', random_state=42)\n",
        "                logistic_model.fit(X_train_logistic, y_logistic_train)\n",
        "                print(\"Fallback to DummyClassifier\")\n",
        "\n",
        "            # Evaluate classification model\n",
        "            try:\n",
        "                logistic_metrics = self._evaluate_logistic_model(logistic_model, X_test_selected, y_logistic_test, selected_feature_names)\n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating classification model: {e}\")\n",
        "                logistic_metrics = {\"error\": str(e)}\n",
        "\n",
        "            # Save models and metadata\n",
        "            try:\n",
        "                joblib.dump(linear_model, os.path.join(self.model_dir, \"linear_model.joblib\"))\n",
        "                joblib.dump(logistic_model, os.path.join(self.model_dir, \"logistic_model.joblib\"))\n",
        "                joblib.dump(scaler, os.path.join(self.model_dir, \"feature_scaler.joblib\"))\n",
        "                joblib.dump(available_features, os.path.join(self.model_dir, \"all_feature_names.joblib\"))\n",
        "                joblib.dump(selected_feature_names, os.path.join(self.model_dir, \"selected_feature_names.joblib\"))\n",
        "                joblib.dump(mask, os.path.join(self.model_dir, \"feature_mask.joblib\"))\n",
        "\n",
        "                # Save metrics\n",
        "                with open(os.path.join(self.model_dir, \"linear_metrics.json\"), 'w') as f:\n",
        "                    json.dump(linear_metrics, f, indent=2)\n",
        "\n",
        "                with open(os.path.join(self.model_dir, \"logistic_metrics.json\"), 'w') as f:\n",
        "                    json.dump(logistic_metrics, f, indent=2)\n",
        "\n",
        "                # Create feature importance visualization if possible\n",
        "                try:\n",
        "                    self._plot_feature_importance(linear_model, logistic_model, selected_feature_names)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error creating feature importance plot: {e}\")\n",
        "\n",
        "                print(f\"✅ Models and metrics saved to {self.model_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving models and metrics: {e}\")\n",
        "\n",
        "            return linear_model, logistic_model, scaler, selected_feature_names, mask\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in train_models: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None, None, None, None, None\n",
        "            \"\"\"\n",
        "            Train both linear and logistic regression models with improved methodology\n",
        "\n",
        "            Parameters:\n",
        "            ticker_symbols (list): List of ticker symbols to use for training\n",
        "\n",
        "            Returns:\n",
        "            tuple: (linear_model, logistic_model, scaler, features)\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # If no tickers specified, use all available merged data files\n",
        "                if ticker_symbols is None:\n",
        "                    merged_files = [f for f in os.listdir(self.processed_dir) if f.endswith('_merged.csv')]\n",
        "                    ticker_symbols = [f.split('_')[0] for f in merged_files]\n",
        "\n",
        "                if not ticker_symbols:\n",
        "                    print(\"No data files found for model training\")\n",
        "                    return None, None, None, None\n",
        "\n",
        "                print(f\"Training models using data from: {ticker_symbols}\")\n",
        "\n",
        "                # Combine data from all tickers\n",
        "                all_data = []\n",
        "                for ticker in ticker_symbols:\n",
        "                    try:\n",
        "                        merged_file = os.path.join(self.processed_dir, f\"{ticker}_merged.csv\")\n",
        "                        if os.path.exists(merged_file):\n",
        "                            ticker_df = pd.read_csv(merged_file)\n",
        "                            ticker_df['ticker'] = ticker\n",
        "                            all_data.append(ticker_df)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading data for {ticker}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                if not all_data:\n",
        "                    print(\"No data available for model training\")\n",
        "                    return None, None, None, None\n",
        "\n",
        "                # Combine all data\n",
        "                combined_df = pd.concat(all_data, ignore_index=True)\n",
        "                combined_df['Date'] = pd.to_datetime(combined_df['Date'], utc=True)\n",
        "\n",
        "                # Define potential features based on what's available\n",
        "                base_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "                # Technical indicator features\n",
        "                tech_features = [col for col in combined_df.columns if col in [\n",
        "                    'sma_5', 'sma_10', 'sma_20', 'ema_5', 'ema_10', 'ema_20',\n",
        "                    'rsi_14', 'macd', 'macd_diff', 'bb_width', 'stoch_k',\n",
        "                    'volatility_daily', 'price_to_sma20', 'gap', 'atr',\n",
        "                    'ema_5_10_cross', 'ema_10_20_cross', 'roc_5', 'close_pct_change'\n",
        "                ]]\n",
        "\n",
        "                # Sentiment features\n",
        "                sentiment_features = [col for col in combined_df.columns if col in [\n",
        "                    'compound_score_mean', 'positive_score_mean', 'negative_score_mean',\n",
        "                    'sentiment_bias', 'sentiment_dispersion', 'post_count',\n",
        "                    'upvotes_sum', 'num_comments_sum', 'engagement_ratio',\n",
        "                    'sent_momentum', 'sent_roll_mean_3', 'sent_price_interaction'\n",
        "                ]]\n",
        "\n",
        "                # Combine all available features\n",
        "                available_features = base_features + tech_features + sentiment_features\n",
        "\n",
        "                # Check if we have enough features\n",
        "                if len(available_features) <= 5:\n",
        "                    print(\"Not enough features available for model training. Using base features only.\")\n",
        "                    available_features = base_features\n",
        "\n",
        "                # Ensure all required columns exist\n",
        "                for feature in available_features:\n",
        "                    if feature not in combined_df.columns:\n",
        "                        print(f\"Warning: {feature} not found in data, adding with zeros\")\n",
        "                        combined_df[feature] = 0\n",
        "\n",
        "                # Handle missing values and infinities\n",
        "                combined_df = combined_df.replace([np.inf, -np.inf], np.nan)\n",
        "                combined_df[available_features] = combined_df[available_features].fillna(0)\n",
        "\n",
        "                # Create additional features\n",
        "                combined_df['price_range'] = combined_df['High'] - combined_df['Low']\n",
        "                combined_df['volume_change'] = combined_df['Volume'].pct_change()\n",
        "                combined_df['close_to_open'] = (combined_df['Close'] - combined_df['Open']) / combined_df['Open'] * 100\n",
        "\n",
        "                # Add these to available features\n",
        "                available_features.extend(['price_range', 'volume_change', 'close_to_open'])\n",
        "\n",
        "                # Create lag features for each ticker separately\n",
        "                for ticker in combined_df['ticker'].unique():\n",
        "                    mask = combined_df['ticker'] == ticker\n",
        "                    for feature in ['Close', 'Volume', 'daily_return']:\n",
        "                        if feature in combined_df.columns:\n",
        "                            combined_df.loc[mask, f'{feature}_lag1'] = combined_df.loc[mask, feature].shift(1)\n",
        "                            combined_df.loc[mask, f'{feature}_lag2'] = combined_df.loc[mask, feature].shift(2)\n",
        "                            combined_df.loc[mask, f'{feature}_lag3'] = combined_df.loc[mask, feature].shift(3)\n",
        "                            available_features.extend([f'{feature}_lag1', f'{feature}_lag2', f'{feature}_lag3'])\n",
        "\n",
        "                # Add sentiment lag features if available\n",
        "                if 'compound_score_mean' in combined_df.columns:\n",
        "                    for ticker in combined_df['ticker'].unique():\n",
        "                        mask = combined_df['ticker'] == ticker\n",
        "                        combined_df.loc[mask, 'sentiment_lag1'] = combined_df.loc[mask, 'compound_score_mean'].shift(1)\n",
        "                        combined_df.loc[mask, 'sentiment_lag2'] = combined_df.loc[mask, 'compound_score_mean'].shift(2)\n",
        "                        available_features.extend(['sentiment_lag1', 'sentiment_lag2'])\n",
        "\n",
        "                # Fill NaN values created by lags\n",
        "                combined_df = combined_df.fillna(0)\n",
        "\n",
        "                # Make feature list unique\n",
        "                available_features = list(set(available_features))\n",
        "\n",
        "                # Create dummy variables for ticker symbols\n",
        "                ticker_dummies = pd.get_dummies(combined_df['ticker'], prefix='ticker')\n",
        "                combined_df = pd.concat([combined_df, ticker_dummies], axis=1)\n",
        "                ticker_features = ticker_dummies.columns.tolist()\n",
        "                available_features.extend(ticker_features)\n",
        "\n",
        "                # Prepare features and targets\n",
        "                X = combined_df[available_features].values\n",
        "                y_linear = combined_df['daily_return'].values\n",
        "                y_logistic = combined_df['price_up_next_day'].values\n",
        "\n",
        "                # Remove any remaining NaN or infinite values\n",
        "                mask = np.isfinite(X).all(axis=1) & np.isfinite(y_linear) & np.isfinite(y_logistic)\n",
        "                X = X[mask]\n",
        "                y_linear = y_linear[mask]\n",
        "                y_logistic = y_logistic[mask]\n",
        "\n",
        "                # Scale features\n",
        "                scaler = StandardScaler()\n",
        "                X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "                # Use time series split for validation\n",
        "                tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "                # Take the last split to have a separate test set\n",
        "                for train_index, test_index in tscv.split(X_scaled):\n",
        "                    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
        "                    y_linear_train, y_linear_test = y_linear[train_index], y_linear[test_index]\n",
        "                    y_logistic_train, y_logistic_test = y_logistic[train_index], y_logistic[test_index]\n",
        "\n",
        "                print(f\"Training data shape: {X_train.shape}\")\n",
        "                print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n",
        "                # Select most relevant features using RandomForest\n",
        "                print(\"Performing feature selection...\")\n",
        "                rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                rf_selector.fit(X_train, y_logistic_train)\n",
        "\n",
        "                # Get feature importances and select top features\n",
        "                importances = rf_selector.feature_importances_\n",
        "                indices = np.argsort(importances)[::-1]\n",
        "\n",
        "                # Print top 20 features\n",
        "                print(\"\\nTop 20 features by importance:\")\n",
        "                for i in range(min(20, X_train.shape[1])):\n",
        "                    print(f\"{available_features[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
        "\n",
        "                # Create a mask to select the top 70% most important features\n",
        "                num_features_to_keep = max(int(X_train.shape[1] * 0.7), 10)  # Keep at least 10 features\n",
        "                mask = np.zeros(X_train.shape[1], dtype=bool)\n",
        "                mask[indices[:num_features_to_keep]] = True\n",
        "\n",
        "                # Filter the features\n",
        "                X_train_selected = X_train[:, mask]\n",
        "                X_test_selected = X_test[:, mask]\n",
        "                selected_feature_names = [available_features[i] for i in range(len(available_features)) if mask[i]]\n",
        "\n",
        "                print(f\"Selected {X_train_selected.shape[1]} of {X_train.shape[1]} features\")\n",
        "\n",
        "                # Check for class imbalance in classification target\n",
        "                class_counts = np.bincount(y_logistic_train)\n",
        "                class_balance = min(class_counts) / max(class_counts)\n",
        "                print(f\"Class distribution: {class_counts}, balance ratio: {class_balance:.2f}\")\n",
        "\n",
        "                # Apply SMOTE for class imbalance if needed\n",
        "                if class_balance < 0.7:\n",
        "                    print(\"Applying SMOTE to balance classes...\")\n",
        "                    try:\n",
        "                        smote = SMOTE(random_state=42)\n",
        "                        X_train_selected_resampled, y_logistic_train_resampled = smote.fit_resample(X_train_selected, y_logistic_train)\n",
        "                        print(f\"After SMOTE: {np.bincount(y_logistic_train_resampled)}\")\n",
        "\n",
        "                        # Use resampled data for classification\n",
        "                        X_train_logistic = X_train_selected_resampled\n",
        "                        y_logistic_train = y_logistic_train_resampled\n",
        "                    except Exception as e:\n",
        "                        print(f\"SMOTE failed: {e}, continuing with original data\")\n",
        "                        X_train_logistic = X_train_selected\n",
        "                else:\n",
        "                    X_train_logistic = X_train_selected\n",
        "\n",
        "                # Train and evaluate models\n",
        "                print(\"\\nTraining regression model for stock returns...\")\n",
        "                regression_models = {\n",
        "                    'ridge': Ridge(alpha=0.01, random_state=42),\n",
        "                    'gbr': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42),\n",
        "                    'xgb': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
        "                }\n",
        "\n",
        "                best_regression_score = -float('inf')\n",
        "                linear_model = None\n",
        "\n",
        "                for name, model in regression_models.items():\n",
        "                    model.fit(X_train_selected, y_linear_train)\n",
        "                    score = model.score(X_train_selected, y_linear_train)\n",
        "                    print(f\"{name} R² score: {score:.4f}\")\n",
        "\n",
        "                    if score > best_regression_score:\n",
        "                        best_regression_score = score\n",
        "                        linear_model = model\n",
        "\n",
        "                linear_metrics = self._evaluate_linear_model(linear_model, X_test_selected, y_linear_test, selected_feature_names)\n",
        "\n",
        "                # Train classification models\n",
        "                print(\"\\nTraining classification model for price direction...\")\n",
        "                classification_models = {\n",
        "                    'logistic': LogisticRegression(C=1.0, class_weight='balanced', penalty='l2',\n",
        "                                                  solver='liblinear', random_state=42, max_iter=1000),\n",
        "                    'random_forest': RandomForestClassifier(n_estimators=100, max_depth=10,\n",
        "                                                          class_weight='balanced', random_state=42),\n",
        "                    'xgb': xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4,\n",
        "                                            use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "                }\n",
        "\n",
        "                best_classification_score = -float('inf')\n",
        "                logistic_model = None\n",
        "\n",
        "                for name, model in classification_models.items():\n",
        "                    model.fit(X_train_logistic, y_logistic_train)\n",
        "\n",
        "                    # For evaluation, use F1 score which is better for imbalanced classes\n",
        "                    from sklearn.metrics import f1_score\n",
        "                    y_pred = model.predict(X_train_selected)\n",
        "                    score = f1_score(y_logistic_train, y_pred)\n",
        "\n",
        "                    print(f\"{name} F1 score: {score:.4f}\")\n",
        "\n",
        "                    if score > best_classification_score:\n",
        "                        best_classification_score = score\n",
        "                        logistic_model = model\n",
        "\n",
        "                logistic_metrics = self._evaluate_logistic_model(logistic_model, X_test_selected, y_logistic_test, selected_feature_names)\n",
        "\n",
        "                # Save models and metadata\n",
        "                joblib.dump(linear_model, os.path.join(self.model_dir, \"linear_model.joblib\"))\n",
        "                joblib.dump(logistic_model, os.path.join(self.model_dir, \"logistic_model.joblib\"))\n",
        "                joblib.dump(scaler, os.path.join(self.model_dir, \"feature_scaler.joblib\"))\n",
        "                joblib.dump(available_features, os.path.join(self.model_dir, \"all_feature_names.joblib\"))\n",
        "                joblib.dump(selected_feature_names, os.path.join(self.model_dir, \"selected_feature_names.joblib\"))\n",
        "                joblib.dump(mask, os.path.join(self.model_dir, \"feature_mask.joblib\"))\n",
        "\n",
        "                # Save metrics\n",
        "                with open(os.path.join(self.model_dir, \"linear_metrics.json\"), 'w') as f:\n",
        "                    json.dump(linear_metrics, f, indent=2)\n",
        "\n",
        "                with open(os.path.join(self.model_dir, \"logistic_metrics.json\"), 'w') as f:\n",
        "                    json.dump(logistic_metrics, f, indent=2)\n",
        "\n",
        "                # Create feature importance visualization\n",
        "                self._plot_feature_importance(linear_model, logistic_model, selected_feature_names)\n",
        "\n",
        "                print(f\"✅ Models and metrics saved to {self.model_dir}\")\n",
        "\n",
        "                return linear_model, logistic_model, scaler, selected_feature_names, mask\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in train_models: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                return None, None, None, None, None\n",
        "\n",
        "    def _train_linear_model(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Train a linear regression model (Ridge) to predict stock returns\n",
        "        \"\"\"\n",
        "        # Simple grid search to find best alpha\n",
        "        alphas = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "        best_score = -float('inf')\n",
        "        best_alpha = 1.0\n",
        "\n",
        "        for alpha in alphas:\n",
        "            model = Ridge(alpha=alpha, random_state=42)\n",
        "            model.fit(X_train, y_train)\n",
        "            score = model.score(X_train, y_train)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_alpha = alpha\n",
        "\n",
        "        print(f\"Best alpha for Ridge Regression: {best_alpha}\")\n",
        "\n",
        "        # Train final model with best alpha\n",
        "        final_model = Ridge(alpha=best_alpha, random_state=42)\n",
        "        final_model.fit(X_train, y_train)\n",
        "\n",
        "        return final_model\n",
        "\n",
        "    def _train_logistic_model(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Train a classification model to predict stock direction\n",
        "        \"\"\"\n",
        "        print(\"Training classification model with multiple algorithms...\")\n",
        "\n",
        "        # Check for class imbalance\n",
        "        class_counts = np.bincount(y_train)\n",
        "        print(f\"Class distribution before balancing: {class_counts}\")\n",
        "\n",
        "        # Apply SMOTE if there's class imbalance\n",
        "        class_balance = min(class_counts) / max(class_counts)\n",
        "        if class_balance < 0.7:\n",
        "            try:\n",
        "                smote = SMOTE(random_state=42)\n",
        "                X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "                print(f\"Applied SMOTE: {X_train.shape} -> {X_train_resampled.shape}\")\n",
        "                print(f\"Balanced class distribution: {np.bincount(y_train_resampled)}\")\n",
        "                X_train, y_train = X_train_resampled, y_train_resampled\n",
        "            except Exception as e:\n",
        "                print(f\"SMOTE failed: {e}, continuing with original data\")\n",
        "\n",
        "        # Try multiple models\n",
        "        models = {\n",
        "            'logistic': LogisticRegression(C=1.0, class_weight='balanced',\n",
        "                                          penalty='l2', solver='liblinear',\n",
        "                                          random_state=42, max_iter=1000),\n",
        "            'random_forest': RandomForestClassifier(n_estimators=100, max_depth=8,\n",
        "                                                  class_weight='balanced',\n",
        "                                                  random_state=42),\n",
        "            'xgboost': xgb.XGBClassifier(n_estimators=100, learning_rate=0.1,\n",
        "                                        max_depth=4, random_state=42,\n",
        "                                        use_label_encoder=False, eval_metric='logloss')\n",
        "        }\n",
        "\n",
        "        # Simple cross-validation to select best model\n",
        "        best_score = 0\n",
        "        best_model = None\n",
        "        best_model_name = None\n",
        "\n",
        "        for name, model in models.items():\n",
        "            print(f\"Training {name}...\")\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Use F1 score for evaluation\n",
        "            from sklearn.metrics import f1_score\n",
        "            y_pred = model.predict(X_train)\n",
        "            score = f1_score(y_train, y_pred)\n",
        "            print(f\"{name} training F1 score: {score:.4f}\")\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_model = model\n",
        "                best_model_name = name\n",
        "\n",
        "        print(f\"Selected {best_model_name} as best classification model with F1 score: {best_score:.4f}\")\n",
        "        return best_model\n",
        "\n",
        "    def _evaluate_linear_model(self, model, X_test, y_test, features):\n",
        "        \"\"\"\n",
        "        Evaluate regression model and generate metrics\n",
        "\n",
        "        Parameters:\n",
        "        model: Trained regression model\n",
        "        X_test: Test feature data\n",
        "        y_test: Test target data\n",
        "        features: List of feature names\n",
        "\n",
        "        Returns:\n",
        "        dict: Evaluation metrics\n",
        "        \"\"\"\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Get directional accuracy (did we predict the direction correctly?)\n",
        "        direction_actual = y_test > 0\n",
        "        direction_pred = y_pred > 0\n",
        "        directional_accuracy = accuracy_score(direction_actual, direction_pred)\n",
        "\n",
        "        print(\"\\n=== Regression Model Evaluation ===\")\n",
        "        print(f\"RMSE: {rmse:.4f}\")\n",
        "        print(f\"MAE: {mae:.4f}\")\n",
        "        print(f\"R²: {r2:.4f}\")\n",
        "        print(f\"Directional Accuracy: {directional_accuracy:.4f}\")\n",
        "\n",
        "        # Get feature importance\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            # For tree-based models\n",
        "            importances = model.feature_importances_\n",
        "        elif hasattr(model, 'coef_'):\n",
        "            # For linear models\n",
        "            importances = np.abs(model.coef_)\n",
        "            coefficients = model.coef_\n",
        "        else:\n",
        "            importances = np.zeros(len(features))\n",
        "            coefficients = np.zeros(len(features))\n",
        "\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': features,\n",
        "            'Importance': importances,\n",
        "            'Coefficient': coefficients if hasattr(model, 'coef_') else importances\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        print(\"\\nTop 10 features by importance (Regression model):\")\n",
        "        print(feature_importance.head(10))\n",
        "\n",
        "        # Create plot of actual vs. predicted\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([-10, 10], [-10, 10], 'r--')\n",
        "        plt.xlabel('Actual Return (%)')\n",
        "        plt.ylabel('Predicted Return (%)')\n",
        "        plt.title('Regression Model: Actual vs. Predicted Returns')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(self.viz_dir, \"regression_actual_vs_predicted.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Create residuals plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        residuals = y_test - y_pred\n",
        "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predicted Return (%)')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title('Regression Model: Residuals Plot')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(self.viz_dir, \"regression_residuals.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Save metrics\n",
        "        metrics = {\n",
        "            'rmse': float(rmse),\n",
        "            'mae': float(mae),\n",
        "            'r2': float(r2),\n",
        "            'directional_accuracy': float(directional_accuracy),\n",
        "            'top_features': feature_importance.head(10).to_dict('records')\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _evaluate_logistic_model(self, model, X_test, y_test, features):\n",
        "        \"\"\"\n",
        "        Evaluate classification model and generate metrics\n",
        "\n",
        "        Parameters:\n",
        "        model: Trained classification model\n",
        "        X_test: Test feature data\n",
        "        y_test: Test target data\n",
        "        features: List of feature names\n",
        "\n",
        "        Returns:\n",
        "        dict: Evaluation metrics\n",
        "        \"\"\"\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Get probability predictions if the model supports it\n",
        "        try:\n",
        "            y_prob = model.predict_proba(X_test)[:, 1]\n",
        "            has_proba = True\n",
        "        except:\n",
        "            y_prob = y_pred\n",
        "            has_proba = False\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(y_test, y_prob) if has_proba else 0\n",
        "        except:\n",
        "            auc = 0\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        print(\"\\n=== Classification Model Evaluation ===\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "        print(f\"AUC-ROC: {auc:.4f}\")\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "        # Get feature importance\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            # For tree-based models\n",
        "            importances = model.feature_importances_\n",
        "            coefficients = importances\n",
        "        elif hasattr(model, 'coef_'):\n",
        "            # For linear models\n",
        "            importances = np.abs(model.coef_[0])\n",
        "            coefficients = model.coef_[0]\n",
        "        else:\n",
        "            importances = np.zeros(len(features))\n",
        "            coefficients = np.zeros(len(features))\n",
        "\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': features,\n",
        "            'Importance': importances,\n",
        "            'Coefficient': coefficients\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        print(\"\\nTop 10 features by importance (Classification model):\")\n",
        "        print(feature_importance.head(10))\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Down', 'Up'],\n",
        "                   yticklabels=['Down', 'Up'])\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.title('Classification Model: Confusion Matrix')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.viz_dir, \"classification_confusion_matrix.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Plot ROC curve if probabilities are available\n",
        "        if has_proba and auc > 0:\n",
        "            from sklearn.metrics import roc_curve\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc:.2f})')\n",
        "            plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "            plt.xlim([0.0, 1.0])\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title('Classification Model: ROC Curve')\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.savefig(os.path.join(self.viz_dir, \"classification_roc_curve.png\"))\n",
        "            plt.close()\n",
        "\n",
        "        # Save metrics\n",
        "        metrics = {\n",
        "            'accuracy': float(accuracy),\n",
        "            'precision': float(precision),\n",
        "            'recall': float(recall),\n",
        "            'f1': float(f1),\n",
        "            'auc': float(auc),\n",
        "            'confusion_matrix': cm.tolist(),\n",
        "            'top_features': feature_importance.head(10).to_dict('records')\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _plot_feature_importance(self, linear_model, logistic_model, features):\n",
        "\n",
        "        \"\"\"\n",
        "        Create visualization comparing feature importance between models\n",
        "\n",
        "        Parameters:\n",
        "        linear_model: Trained regression model\n",
        "        logistic_model: Trained classification model\n",
        "        features: List of feature names\n",
        "        \"\"\"\n",
        "        # Get feature importance for both models\n",
        "        if hasattr(linear_model, 'feature_importances_'):\n",
        "            linear_importances = linear_model.feature_importances_\n",
        "        elif hasattr(linear_model, 'coef_'):\n",
        "            linear_importances = np.abs(linear_model.coef_)\n",
        "        else:\n",
        "            linear_importances = np.zeros(len(features))\n",
        "\n",
        "        linear_importance_df = pd.DataFrame({\n",
        "            'Feature': features,\n",
        "            'Importance': linear_importances,\n",
        "            'Model': 'Regression'\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        if hasattr(logistic_model, 'feature_importances_'):\n",
        "            logistic_importances = logistic_model.feature_importances_\n",
        "        elif hasattr(logistic_model, 'coef_'):\n",
        "            try:\n",
        "                logistic_importances = np.abs(logistic_model.coef_[0])\n",
        "            except:\n",
        "                logistic_importances = np.abs(logistic_model.coef_)\n",
        "        else:\n",
        "            logistic_importances = np.zeros(len(features))\n",
        "\n",
        "        logistic_importance_df = pd.DataFrame({\n",
        "            'Feature': features,\n",
        "            'Importance': logistic_importances,\n",
        "            'Model': 'Classification'\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        # Combine and normalize for comparison\n",
        "        combined = pd.concat([linear_importance_df, logistic_importance_df])\n",
        "\n",
        "        # Normalize importances within each model\n",
        "        for model in ['Regression', 'Classification']:\n",
        "            mask = combined['Model'] == model\n",
        "            max_importance = combined.loc[mask, 'Importance'].max()\n",
        "            if max_importance > 0:\n",
        "                combined.loc[mask, 'Importance'] = combined.loc[mask, 'Importance'] / max_importance\n",
        "\n",
        "        # Get top 15 features from both models\n",
        "        top_features = set(linear_importance_df.head(15)['Feature']).union(\n",
        "            set(logistic_importance_df.head(15)['Feature']))\n",
        "\n",
        "        # Filter to only top features\n",
        "        plot_data = combined[combined['Feature'].isin(top_features)]\n",
        "\n",
        "        # Create grouped bar chart\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        sns.set_style(\"whitegrid\")\n",
        "        chart = sns.barplot(x='Feature', y='Importance', hue='Model', data=plot_data)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.title('Feature Importance Comparison: Regression vs. Classification Model', fontsize=14)\n",
        "        plt.xlabel('Feature', fontsize=12)\n",
        "        plt.ylabel('Normalized Importance', fontsize=12)\n",
        "        plt.legend(title='Model Type')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.viz_dir, \"feature_importance_comparison.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Create individual feature importance plots for each model\n",
        "        for model_type, df in [('Regression', linear_importance_df), ('Classification', logistic_importance_df)]:\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            top_n = min(20, len(df))\n",
        "            sns.barplot(x='Importance', y='Feature', data=df.head(top_n), palette='viridis')\n",
        "            plt.title(f'Top {top_n} Features for {model_type} Model', fontsize=14)\n",
        "            plt.xlabel('Importance', fontsize=12)\n",
        "            plt.ylabel('Feature', fontsize=12)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.viz_dir, f\"{model_type.lower()}_feature_importance.png\"))\n",
        "            plt.close()\n"
      ],
      "metadata": {
        "id": "4SM4qilKVrFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_analysis(tickers, days_back=180, reddit_limit=500):\n",
        "    \"\"\"\n",
        "    Run the complete analysis workflow for a list of tickers\n",
        "\n",
        "    Parameters:\n",
        "    tickers (list): List of stock ticker symbols\n",
        "    days_back (int): Number of days to look back for data\n",
        "    reddit_limit (int): Maximum number of Reddit posts to fetch per subreddit\n",
        "\n",
        "    Returns:\n",
        "    tuple: (analyzer, models_dict) or (None, None) if analysis fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize analyzer\n",
        "        analyzer = StockSentimentAnalyzer()\n",
        "        successful_tickers = []\n",
        "\n",
        "        # Process each ticker\n",
        "        for ticker in tickers:\n",
        "            print(f\"\\n{'='*50}\\nProcessing {ticker}\\n{'='*50}\")\n",
        "            try:\n",
        "                # 1. Fetch stock data\n",
        "                stock_data = analyzer.fetch_stock_data(ticker, period=f\"{days_back}d\")\n",
        "                if stock_data is None:\n",
        "                    print(f\"Skipping {ticker} due to missing stock data\")\n",
        "                    continue\n",
        "\n",
        "                # 2. Fetch Reddit data\n",
        "                reddit_data = analyzer.fetch_reddit_data(\n",
        "                    ticker,\n",
        "                    subreddits=[\"stocks\", \"investing\", \"wallstreetbets\", \"stockmarket\"],\n",
        "                    limit=reddit_limit,\n",
        "                    days_back=days_back\n",
        "                )\n",
        "\n",
        "                # 3. Merge data\n",
        "                merged_data = analyzer.merge_stock_and_sentiment(ticker)\n",
        "                if merged_data is None:\n",
        "                    print(f\"Skipping {ticker} due to issues with data merging\")\n",
        "                    continue\n",
        "\n",
        "                # 4. Analyze data\n",
        "                stats = analyzer.analyze_data(ticker)\n",
        "                if stats is not None:\n",
        "                    successful_tickers.append(ticker)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {ticker}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Only train models if we have successful data\n",
        "        if successful_tickers:\n",
        "            try:\n",
        "                # 5. Train models using successful tickers\n",
        "                linear_model, logistic_model, scaler, features, feature_mask = analyzer.train_models(successful_tickers)\n",
        "\n",
        "                models_dict = {\n",
        "                    'linear_model': linear_model,\n",
        "                    'logistic_model': logistic_model,\n",
        "                    'scaler': scaler,\n",
        "                    'features': features,\n",
        "                    'feature_mask': feature_mask\n",
        "                }\n",
        "\n",
        "                return analyzer, models_dict\n",
        "            except Exception as e:\n",
        "                print(f\"Error training models: {str(e)}\")\n",
        "                return analyzer, None\n",
        "        else:\n",
        "            print(\"No successful ticker analysis to train models\")\n",
        "            return analyzer, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal error in run_analysis: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# Web application for the project\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "# Global variables to store the analyzer and models\n",
        "global_analyzer = None\n",
        "global_models = None\n",
        "\n",
        "def initialize_analyzer():\n",
        "    \"\"\"Initialize the analyzer and models if not already done\"\"\"\n",
        "    global global_analyzer, global_models\n",
        "    if global_analyzer is None:\n",
        "        global_analyzer, global_models = run_analysis(\n",
        "            tickers=[\"AAPL\", \"MSFT\", \"TSLA\", \"AMZN\", \"NVDA\"],\n",
        "            days_back=30,\n",
        "            reddit_limit=100\n",
        "        )\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    \"\"\"Render the home page\"\"\"\n",
        "    initialize_analyzer()\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "def analyze():\n",
        "    \"\"\"Handle stock analysis request\"\"\"\n",
        "    try:\n",
        "        ticker = request.form.get('ticker', '').upper()\n",
        "        if not ticker:\n",
        "            return jsonify({'error': 'No ticker provided'})\n",
        "\n",
        "        print(f\"Processing analysis request for {ticker}\")\n",
        "\n",
        "        # Initialize analyzer if not already done\n",
        "        initialize_analyzer()\n",
        "\n",
        "        if global_analyzer is None:\n",
        "            return jsonify({'error': 'Failed to initialize analyzer'})\n",
        "\n",
        "        # Fetch new data for the ticker\n",
        "        stock_data = global_analyzer.fetch_stock_data(ticker)\n",
        "        if stock_data is None:\n",
        "            return jsonify({'error': f'Failed to fetch stock data for {ticker}'})\n",
        "\n",
        "        reddit_data = global_analyzer.fetch_reddit_data(ticker)\n",
        "        merged_data = global_analyzer.merge_stock_and_sentiment(ticker)\n",
        "        stats = global_analyzer.analyze_data(ticker)\n",
        "\n",
        "        if merged_data is None or stats is None:\n",
        "            return jsonify({'error': f'Failed to analyze {ticker}'})\n",
        "\n",
        "        # Create interactive chart using Plotly\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # Add candlestick chart\n",
        "        fig.add_trace(go.Candlestick(\n",
        "            x=merged_data['Date'],\n",
        "            open=merged_data['Open'],\n",
        "            high=merged_data['High'],\n",
        "            low=merged_data['Low'],\n",
        "            close=merged_data['Close'],\n",
        "            name='OHLC'\n",
        "        ))\n",
        "\n",
        "        # Add moving averages if available\n",
        "        if 'sma_20' in merged_data.columns:\n",
        "            # Clean sma_20 data - remove NaN and inf values\n",
        "            sma_20_clean = merged_data['sma_20'].replace([np.inf, -np.inf], np.nan)\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=merged_data['Date'],\n",
        "                y=sma_20_clean,\n",
        "                name='20-day MA',\n",
        "                line=dict(color='purple', width=1.5)\n",
        "            ))\n",
        "\n",
        "        if 'ema_10' in merged_data.columns:\n",
        "            # Clean ema_10 data\n",
        "            ema_10_clean = merged_data['ema_10'].replace([np.inf, -np.inf], np.nan)\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=merged_data['Date'],\n",
        "                y=ema_10_clean,\n",
        "                name='10-day EMA',\n",
        "                line=dict(color='orange', width=1.5),\n",
        "                visible='legendonly'  # Hidden by default\n",
        "            ))\n",
        "\n",
        "        # Add sentiment overlay if available\n",
        "        if 'compound_score_mean' in merged_data.columns:\n",
        "            # Clean sentiment data\n",
        "            sentiment_clean = merged_data['compound_score_mean'].replace([np.inf, -np.inf], np.nan)\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=merged_data['Date'],\n",
        "                y=sentiment_clean,\n",
        "                name='Sentiment Score',\n",
        "                yaxis='y2',\n",
        "                line=dict(color='green', width=2, dash='dash')\n",
        "            ))\n",
        "\n",
        "            # Add post counts as bubbles if available\n",
        "            if 'post_count' in merged_data.columns:\n",
        "                # Clean post_count data and create safe bubble sizes\n",
        "                post_count_clean = merged_data['post_count'].fillna(0).replace([np.inf, -np.inf], 0)\n",
        "\n",
        "                # Create bubble size with minimum threshold\n",
        "                bubble_size = post_count_clean * 5  # Scale for visibility\n",
        "\n",
        "                # Set minimum size and maximum size to prevent extremes\n",
        "                bubble_size = bubble_size.clip(lower=5, upper=50)\n",
        "\n",
        "                # Only show bubbles where we actually have posts\n",
        "                mask = post_count_clean > 0\n",
        "\n",
        "                if mask.any():  # Only add trace if we have valid data\n",
        "                    fig.add_trace(go.Scatter(\n",
        "                        x=merged_data['Date'][mask],\n",
        "                        y=sentiment_clean[mask],\n",
        "                        mode='markers',\n",
        "                        marker=dict(\n",
        "                            size=bubble_size[mask],\n",
        "                            color='rgba(0, 100, 80, 0.5)',\n",
        "                            line=dict(width=1, color='rgba(0, 100, 80, 1)')\n",
        "                        ),\n",
        "                        name='Post Volume',\n",
        "                        yaxis='y2',\n",
        "                        hovertemplate='Date: %{x}<br>Posts: %{text}<br>Sentiment: %{y:.2f}<extra></extra>',\n",
        "                        text=post_count_clean[mask]\n",
        "                    ))\n",
        "\n",
        "        # Update layout with custom design\n",
        "        fig.update_layout(\n",
        "            title=f'{ticker} Stock Price and Sentiment Analysis',\n",
        "            yaxis=dict(\n",
        "                title='Stock Price ($)',\n",
        "                titlefont=dict(color='black'),\n",
        "                tickfont=dict(color='black')\n",
        "            ),\n",
        "            yaxis2=dict(\n",
        "                title='Sentiment Score',\n",
        "                titlefont=dict(color='green'),\n",
        "                tickfont=dict(color='green'),\n",
        "                overlaying='y',\n",
        "                side='right',\n",
        "                range=[-1, 1],\n",
        "                showgrid=False\n",
        "            ),\n",
        "            xaxis_title='Date',\n",
        "            legend=dict(\n",
        "                orientation=\"h\",\n",
        "                yanchor=\"bottom\",\n",
        "                y=0.15,\n",
        "                xanchor=\"right\",\n",
        "                x=1\n",
        "            ),\n",
        "            template='plotly_white',\n",
        "            height=800,\n",
        "            margin=dict(l=70, r=100, t=80, b=100),\n",
        "            hovermode='x unified',\n",
        "            showlegend=True,\n",
        "            autosize=True\n",
        "        )\n",
        "\n",
        "        # Add volume as a bar chart at the bottom\n",
        "        fig2 = go.Figure()\n",
        "\n",
        "        # Clean volume data\n",
        "        volume_clean = merged_data['Volume'].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "        fig2.add_trace(go.Bar(\n",
        "            x=merged_data['Date'],\n",
        "            y=volume_clean,\n",
        "            name='Volume',\n",
        "            marker_color='rgba(100, 100, 200, 0.7)'\n",
        "        ))\n",
        "\n",
        "        # Add RSI indicator if available\n",
        "        if 'rsi_14' in merged_data.columns:\n",
        "            fig3 = go.Figure()\n",
        "\n",
        "            # Clean RSI data\n",
        "            rsi_clean = merged_data['rsi_14'].replace([np.inf, -np.inf], np.nan)\n",
        "            fig3.add_trace(go.Scatter(\n",
        "                x=merged_data['Date'],\n",
        "                y=rsi_clean,\n",
        "                name='RSI (14)',\n",
        "                line=dict(color='purple', width=1.5)\n",
        "            ))\n",
        "\n",
        "            # Add RSI reference lines\n",
        "            fig3.add_shape(\n",
        "                type=\"line\",\n",
        "                x0=merged_data['Date'].min(),\n",
        "                y0=70,\n",
        "                x1=merged_data['Date'].max(),\n",
        "                y1=70,\n",
        "                line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
        "            )\n",
        "\n",
        "            fig3.add_shape(\n",
        "                type=\"line\",\n",
        "                x0=merged_data['Date'].min(),\n",
        "                y0=30,\n",
        "                x1=merged_data['Date'].max(),\n",
        "                y1=30,\n",
        "                line=dict(color=\"green\", width=1, dash=\"dash\"),\n",
        "            )\n",
        "\n",
        "            fig3.update_layout(\n",
        "                title=\"RSI (14-day)\",\n",
        "                yaxis=dict(\n",
        "                    title='RSI',\n",
        "                    range=[0, 100]\n",
        "                ),\n",
        "                height=200,\n",
        "                margin=dict(l=50, r=10, t=30, b=10),\n",
        "                showlegend=False\n",
        "            )\n",
        "        else:\n",
        "            fig3 = None\n",
        "\n",
        "        # Update Volume chart layout\n",
        "        fig2.update_layout(\n",
        "            title=\"Trading Volume\",\n",
        "            yaxis=dict(title='Volume'),\n",
        "            height=200,\n",
        "            margin=dict(l=50, r=10, t=30, b=30),\n",
        "            showlegend=False\n",
        "        )\n",
        "\n",
        "        # Generate predictions\n",
        "        predictions = None\n",
        "        if global_models and global_models.get('features') and global_models.get('feature_mask') is not None:\n",
        "            try:\n",
        "                # Create feature vector with all available features\n",
        "                required_features = global_models['features']\n",
        "                all_features = joblib.load(os.path.join(global_analyzer.model_dir, \"all_feature_names.joblib\"))\n",
        "                feature_mask = global_models['feature_mask']\n",
        "\n",
        "                # Prepare the latest data point for prediction\n",
        "                feature_row = {}\n",
        "\n",
        "                # Start with all zeros\n",
        "                for feature in all_features:\n",
        "                    feature_row[feature] = 0\n",
        "\n",
        "                # Fill in the values we have from merged_data\n",
        "                for feature in all_features:\n",
        "                    if feature in merged_data.columns:\n",
        "                        # Clean the value before using it\n",
        "                        value = merged_data[feature].iloc[-1]\n",
        "                        if pd.isna(value) or np.isinf(value):\n",
        "                            value = 0\n",
        "                        feature_row[feature] = float(value)\n",
        "\n",
        "                # Create dummy variables for the ticker\n",
        "                for col in all_features:\n",
        "                    if col.startswith('ticker_') and col == f'ticker_{ticker}':\n",
        "                        feature_row[col] = 1\n",
        "\n",
        "                # Convert to numpy array in the correct order\n",
        "                feature_vector = np.array([feature_row[f] for f in all_features]).reshape(1, -1)\n",
        "\n",
        "                # Check for any remaining NaN or inf values\n",
        "                feature_vector = np.nan_to_num(feature_vector, nan=0, posinf=0, neginf=0)\n",
        "\n",
        "                # Scale the features\n",
        "                feature_vector_scaled = global_models['scaler'].transform(feature_vector)\n",
        "\n",
        "                # Apply feature mask for selected features\n",
        "                feature_vector_selected = feature_vector_scaled[:, feature_mask]\n",
        "\n",
        "                # Generate predictions\n",
        "                if global_models['linear_model'] is not None:\n",
        "                    linear_pred = global_models['linear_model'].predict(feature_vector_selected)\n",
        "                    linear_return = float(linear_pred[0])\n",
        "                else:\n",
        "                    linear_return = 0\n",
        "\n",
        "                if global_models['logistic_model'] is not None:\n",
        "                    try:\n",
        "                        logistic_prob = global_models['logistic_model'].predict_proba(feature_vector_selected)[:, 1]\n",
        "                        prob_up = float(logistic_prob[0])\n",
        "                    except:\n",
        "                        prob_up = float(global_models['logistic_model'].predict(feature_vector_selected)[0])\n",
        "                else:\n",
        "                    prob_up = 0.5\n",
        "\n",
        "                predictions = {\n",
        "                    'returns': linear_return,\n",
        "                    'probability_up': prob_up\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating predictions: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                predictions = {'error': str(e)}\n",
        "\n",
        "        # Prepare the final response\n",
        "        response_data = {\n",
        "            'success': True,\n",
        "            'stats': stats,\n",
        "            'predictions': predictions,\n",
        "            'price_chart': json.loads(fig.to_json()),\n",
        "            'volume_chart': json.loads(fig2.to_json()),\n",
        "            'rsi_chart': json.loads(fig3.to_json()) if fig3 else None\n",
        "        }\n",
        "\n",
        "        print(f\"Analysis completed for {ticker}\")\n",
        "        return jsonify(response_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in analyze(): {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({'error': str(e)})\n",
        "    \"\"\"Handle stock analysis request\"\"\"\n",
        "    try:\n",
        "        ticker = request.form.get('ticker', '').upper()\n",
        "        if not ticker:\n",
        "            return jsonify({'error': 'No ticker provided'})\n",
        "\n",
        "        print(f\"Processing analysis request for {ticker}\")\n",
        "\n",
        "        # Initialize analyzer if not already done\n",
        "        initialize_analyzer()\n",
        "\n",
        "        if global_analyzer is None:\n",
        "            return jsonify({'error': 'Failed to initialize analyzer'})\n",
        "\n",
        "        # Fetch new data for the ticker\n",
        "        stock_data = global_analyzer.fetch_stock_data(ticker)\n",
        "        if stock_data is None:\n",
        "            return jsonify({'error': f'Failed to fetch stock data for {ticker}'})\n",
        "\n",
        "        reddit_data = global_analyzer.fetch_reddit_data(ticker)\n",
        "        merged_data = global_analyzer.merge_stock_and_sentiment(ticker)\n",
        "        stats = global_analyzer.analyze_data(ticker)\n",
        "\n",
        "        if merged_data is None or stats is None:\n",
        "            return jsonify({'error': f'Failed to analyze {ticker}'})\n",
        "\n",
        "        # Create interactive chart using Plotly\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # Add candlestick chart\n",
        "        fig.add_trace(go.Candlestick(\n",
        "            x=merged_data['Date'],\n",
        "            open=merged_data['Open'],\n",
        "            high=merged_data['High'],\n",
        "            low=merged_data['Low'],\n",
        "            close=merged_data['Close'],\n",
        "            name='OHLC'\n",
        "        ))\n",
        "\n",
        "        # Add moving averages if available\n",
        "        if 'sma_20' in merged_data.columns:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=merged_data['Date'],\n",
        "                y=merged_data['sma_20'],\n",
        "                name='20-day MA',\n",
        "                line=dict(color='purple', width=1.5)\n",
        "            ))\n",
        "\n",
        "        if 'ema_10' in merged_data.columns:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=merged_data['Date'],\n",
        "                y=merged_data['ema_10'],\n",
        "                name='10-day EMA',\n",
        "                line=dict(color='orange', width=1.5),\n",
        "                visible='legendonly'  # Hidden by default\n",
        "            ))\n",
        "\n",
        "        # Add sentiment overlay if available\n",
        "        if 'compound_score_mean' in merged_data.columns:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=merged_data['Date'],\n",
        "                y=merged_data['compound_score_mean'],\n",
        "                name='Sentiment Score',\n",
        "                yaxis='y2',\n",
        "                line=dict(color='green', width=2, dash='dash')\n",
        "            ))\n",
        "\n",
        "            # Add post counts as bubbles if available\n",
        "            if 'post_count' in merged_data.columns:\n",
        "                bubble_size = merged_data['post_count'] * 5  # Scale for visibility\n",
        "                bubble_size = bubble_size.fillna(0).replace(0, np.nan)  # Only show when count > 0\n",
        "\n",
        "\n",
        "\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=merged_data['Date'],\n",
        "                    y=merged_data['compound_score_mean'],\n",
        "                    mode='markers',\n",
        "                    marker=dict(\n",
        "                        size=bubble_size,\n",
        "                        color='rgba(0, 100, 80, 0.5)',\n",
        "                        line=dict(width=1, color='rgba(0, 100, 80, 1)')\n",
        "                    ),\n",
        "                    name='Post Volume',\n",
        "                    yaxis='y2',\n",
        "                    hovertemplate='Date: %{x}<br>Posts: %{text}<br>Sentiment: %{y:.2f}<extra></extra>',\n",
        "                    text=merged_data['post_count']\n",
        "                ))\n",
        "\n",
        "        # Update layout with custom design\n",
        "        fig.update_layout(\n",
        "          title=f'{ticker} Stock Price and Sentiment Analysis',\n",
        "          autosize=True,  # Allow automatic sizing\n",
        "          height=600,     # Optional: adjust height as needed\n",
        "          margin=dict(l=60, r=60, t=60, b=60),\n",
        "          xaxis=dict(\n",
        "              title='Date',\n",
        "              automargin=True\n",
        "          ),\n",
        "          yaxis=dict(\n",
        "              title='Stock Price ($)',\n",
        "              titlefont=dict(color='black'),\n",
        "              tickfont=dict(color='black'),\n",
        "              automargin=True\n",
        "          ),\n",
        "          yaxis2=dict(\n",
        "              title='Sentiment Score',\n",
        "              titlefont=dict(color='green'),\n",
        "              tickfont=dict(color='green'),\n",
        "              overlaying='y',\n",
        "              side='right',\n",
        "              range=[-1, 1],\n",
        "              showgrid=False\n",
        "          ),\n",
        "          legend=dict(\n",
        "              orientation=\"h\",\n",
        "              yanchor=\"bottom\",\n",
        "              y=1.02,\n",
        "              xanchor=\"right\",\n",
        "              x=1\n",
        "          ),\n",
        "          template='plotly_white',\n",
        "          hovermode='x unified'\n",
        "      )\n",
        "        # Add volume as a bar chart at the bottom\n",
        "        fig2 = go.Figure()\n",
        "\n",
        "        fig2.add_trace(go.Bar(\n",
        "            x=merged_data['Date'],\n",
        "            y=merged_data['Volume'],\n",
        "            name='Volume',\n",
        "            marker_color='rgba(100, 100, 200, 0.7)'\n",
        "        ))\n",
        "\n",
        "        # Add RSI indicator if available\n",
        "        if 'rsi_14' in merged_data.columns:\n",
        "            fig3 = go.Figure()\n",
        "\n",
        "            fig3.add_trace(go.Scatter(\n",
        "                x=merged_data['Date'],\n",
        "                y=merged_data['rsi_14'],\n",
        "                name='RSI (14)',\n",
        "                line=dict(color='purple', width=1.5)\n",
        "            ))\n",
        "\n",
        "            # Add RSI reference lines\n",
        "            fig3.add_shape(\n",
        "                type=\"line\",\n",
        "                x0=merged_data['Date'].min(),\n",
        "                y0=70,\n",
        "                x1=merged_data['Date'].max(),\n",
        "                y1=70,\n",
        "                line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
        "            )\n",
        "\n",
        "            fig3.add_shape(\n",
        "                type=\"line\",\n",
        "                x0=merged_data['Date'].min(),\n",
        "                y0=30,\n",
        "                x1=merged_data['Date'].max(),\n",
        "                y1=30,\n",
        "                line=dict(color=\"green\", width=1, dash=\"dash\"),\n",
        "            )\n",
        "\n",
        "            fig3.update_layout(\n",
        "                title=\"RSI (14-day)\",\n",
        "                yaxis=dict(\n",
        "                    title='RSI',\n",
        "                    range=[0, 100]\n",
        "                ),\n",
        "                height=200,\n",
        "                margin=dict(l=50, r=10, t=30, b=10),\n",
        "                showlegend=False\n",
        "            )\n",
        "        else:\n",
        "            fig3 = None\n",
        "\n",
        "        # Update Volume chart layout\n",
        "        fig2.update_layout(\n",
        "            title=\"Trading Volume\",\n",
        "            yaxis=dict(title='Volume'),\n",
        "            height=300,\n",
        "            margin=dict(l=50, r=10, t=30, b=30),\n",
        "            showlegend=False\n",
        "        )\n",
        "\n",
        "        # Generate predictions\n",
        "        predictions = None\n",
        "        if global_models and global_models.get('features') and global_models.get('feature_mask') is not None:\n",
        "            try:\n",
        "                # Create feature vector with all available features\n",
        "                required_features = global_models['features']\n",
        "                all_features = joblib.load(os.path.join(global_analyzer.model_dir, \"all_feature_names.joblib\"))\n",
        "                feature_mask = global_models['feature_mask']\n",
        "\n",
        "                # Prepare the latest data point for prediction\n",
        "                feature_row = {}\n",
        "\n",
        "                # Start with all zeros\n",
        "                for feature in all_features:\n",
        "                    feature_row[feature] = 0\n",
        "\n",
        "                # Fill in the values we have from merged_data\n",
        "                for feature in all_features:\n",
        "                    if feature in merged_data.columns:\n",
        "                        feature_row[feature] = float(merged_data[feature].iloc[-1])\n",
        "\n",
        "                # Create dummy variables for the ticker\n",
        "                for col in all_features:\n",
        "                    if col.startswith('ticker_') and col == f'ticker_{ticker}':\n",
        "                        feature_row[col] = 1\n",
        "\n",
        "                # Convert to numpy array in the correct order\n",
        "                feature_vector = np.array([feature_row[f] for f in all_features]).reshape(1, -1)\n",
        "\n",
        "                # Scale the features\n",
        "                feature_vector_scaled = global_models['scaler'].transform(feature_vector)\n",
        "\n",
        "                # Apply feature mask for selected features\n",
        "                feature_vector_selected = feature_vector_scaled[:, feature_mask]\n",
        "\n",
        "                # Generate predictions\n",
        "                if global_models['linear_model'] is not None:\n",
        "                    linear_pred = global_models['linear_model'].predict(feature_vector_selected)\n",
        "                    linear_return = float(linear_pred[0])\n",
        "                else:\n",
        "                    linear_return = 0\n",
        "\n",
        "                if global_models['logistic_model'] is not None:\n",
        "                    try:\n",
        "                        logistic_prob = global_models['logistic_model'].predict_proba(feature_vector_selected)[:, 1]\n",
        "                        prob_up = float(logistic_prob[0])\n",
        "                    except:\n",
        "                        prob_up = float(global_models['logistic_model'].predict(feature_vector_selected)[0])\n",
        "                else:\n",
        "                    prob_up = 0.5\n",
        "\n",
        "                predictions = {\n",
        "                    'returns': linear_return,\n",
        "                    'probability_up': prob_up\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating predictions: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                predictions = {'error': str(e)}\n",
        "\n",
        "        # Prepare the final response\n",
        "        response_data = {\n",
        "            'success': True,\n",
        "            'stats': stats,\n",
        "            'predictions': predictions,\n",
        "            'price_chart': json.loads(fig.to_json()),\n",
        "            'volume_chart': json.loads(fig2.to_json()),\n",
        "            'rsi_chart': json.loads(fig3.to_json()) if fig3 else None\n",
        "        }\n",
        "\n",
        "        print(f\"Analysis completed for {ticker}\")\n",
        "        return jsonify(response_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in analyze(): {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({'error': str(e)})\n",
        "\n",
        "# Create the templates directory and HTML template\n",
        "def create_template_directory():\n",
        "    \"\"\"Create templates directory and index.html\"\"\"\n",
        "    import os\n",
        "    os.makedirs('templates', exist_ok=True)\n",
        "    with open('templates/index.html', 'w') as f:\n",
        "        f.write(INDEX_HTML)\n",
        "\n",
        "\n",
        "# HTML template for the web application\n",
        "INDEX_HTML = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Trading On Trends</title>\n",
        "    <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
        "    <script src=\"https://code.jquery.com/jquery-3.6.0.min.js\"></script>\n",
        "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js\"></script>\n",
        "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js\"></script>\n",
        "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\">\n",
        "    <style>\n",
        "        @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap');\n",
        "\n",
        "        :root {\n",
        "            --primary: #6c5ce7;\n",
        "            --secondary: #a29bfe;\n",
        "            --success: #00cec9;\n",
        "            --danger: #e74c3c;\n",
        "            --warning: #fdcb6e;\n",
        "            --text: #2d3436;\n",
        "            --light: #f9f9f9;\n",
        "            --dark: #262837;\n",
        "            --card-bg: rgba(255, 255, 255, 0.9);\n",
        "            --card-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);\n",
        "            --gradient: linear-gradient(135deg, #6c5ce7, #00cec9);\n",
        "        }\n",
        "\n",
        "        * {\n",
        "            margin: 0;\n",
        "            padding: 0;\n",
        "            box-sizing: border-box;\n",
        "        }\n",
        "\n",
        "        body {\n",
        "            font-family: 'Poppins', sans-serif;\n",
        "            background-color: var(--light);\n",
        "            color: var(--text);\n",
        "            overflow-x: hidden;\n",
        "        }\n",
        "\n",
        "        canvas {\n",
        "            position: fixed;\n",
        "            top: 0;\n",
        "            left: 0;\n",
        "            width: 100%;\n",
        "            height: 100%;\n",
        "            z-index: -1;\n",
        "        }\n",
        "\n",
        "        .container {\n",
        "            width: 100%;\n",
        "            min-height: 100vh;\n",
        "            display: flex;\n",
        "            justify-content: center;\n",
        "            align-items: center;\n",
        "            padding: 20px;\n",
        "        }\n",
        "\n",
        "        .app-wrapper {\n",
        "            width: 100%;\n",
        "            max-width: 1200px;\n",
        "            background: var(--card-bg);\n",
        "            border-radius: 24px;\n",
        "            box-shadow: var(--card-shadow);\n",
        "            padding: 40px;\n",
        "            position: relative;\n",
        "            overflow: hidden;\n",
        "            backdrop-filter: blur(10px);\n",
        "        }\n",
        "\n",
        "        .app-header {\n",
        "            text-align: center;\n",
        "            margin-bottom: 40px;\n",
        "            transform: translateY(30px);\n",
        "            opacity: 0;\n",
        "        }\n",
        "\n",
        "        .app-header h1 {\n",
        "            font-size: 2.5rem;\n",
        "            font-weight: 700;\n",
        "            margin-bottom: 10px;\n",
        "            background: var(--gradient);\n",
        "            -webkit-background-clip: text;\n",
        "            -webkit-text-fill-color: transparent;\n",
        "        }\n",
        "\n",
        "        .app-header p {\n",
        "            color: var(--text);\n",
        "            opacity: 0.7;\n",
        "        }\n",
        "\n",
        "        .search-box {\n",
        "            display: flex;\n",
        "            justify-content: center;\n",
        "            margin-bottom: 40px;\n",
        "            transform: translateY(30px);\n",
        "            opacity: 0;\n",
        "        }\n",
        "\n",
        "        .search-container {\n",
        "            position: relative;\n",
        "            width: 100%;\n",
        "            max-width: 500px;\n",
        "        }\n",
        "\n",
        "        .search-container input {\n",
        "            width: 100%;\n",
        "            padding: 18px 24px;\n",
        "            padding-right: 70px;\n",
        "            background: white;\n",
        "            border: none;\n",
        "            border-radius: 50px;\n",
        "            font-size: 1rem;\n",
        "            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);\n",
        "            transition: all 0.3s ease;\n",
        "        }\n",
        "\n",
        "        .search-container input:focus {\n",
        "            outline: none;\n",
        "            box-shadow: 0 4px 20px rgba(108, 92, 231, 0.25);\n",
        "        }\n",
        "\n",
        "        .search-btn {\n",
        "            position: absolute;\n",
        "            top: 5px;\n",
        "            right: 5px;\n",
        "            height: 44px;\n",
        "            width: 44px;\n",
        "            border-radius: 50%;\n",
        "            background: var(--gradient);\n",
        "            border: none;\n",
        "            color: white;\n",
        "            cursor: pointer;\n",
        "            font-size: 16px;\n",
        "            transition: all 0.3s ease;\n",
        "        }\n",
        "\n",
        "        .search-btn:hover {\n",
        "            transform: scale(1.05);\n",
        "        }\n",
        "\n",
        "        #loading {\n",
        "            display: none;\n",
        "            text-align: center;\n",
        "            margin: 20px 0;\n",
        "        }\n",
        "\n",
        "        .loader {\n",
        "            display: inline-block;\n",
        "            width: 80px;\n",
        "            height: 80px;\n",
        "        }\n",
        "\n",
        "        .loader:after {\n",
        "            content: \" \";\n",
        "            display: block;\n",
        "            width: 64px;\n",
        "            height: 64px;\n",
        "            margin: 8px;\n",
        "            border-radius: 50%;\n",
        "            border: 6px solid var(--primary);\n",
        "            border-color: var(--primary) transparent var(--primary) transparent;\n",
        "            animation: loader 1.2s linear infinite;\n",
        "        }\n",
        "\n",
        "        @keyframes loader {\n",
        "            0% {\n",
        "                transform: rotate(0deg);\n",
        "            }\n",
        "            100% {\n",
        "                transform: rotate(360deg);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        #results {\n",
        "            opacity: 0;\n",
        "            transform: translateY(30px);\n",
        "        }\n",
        "\n",
        "        .chart-container {\n",
        "            margin-bottom: 30px;\n",
        "            border-radius: 16px;\n",
        "            overflow: hidden;\n",
        "            box-shadow: var(--card-shadow);\n",
        "            background: white;\n",
        "        }\n",
        "\n",
        "        #price-chart, #volume-chart, #rsi-chart {\n",
        "            width: 100%;\n",
        "            background: white;\n",
        "        }\n",
        "\n",
        "        #price-chart {\n",
        "            height: 500px;\n",
        "        }\n",
        "\n",
        "        #volume-chart, #rsi-chart {\n",
        "            height: 200px;\n",
        "        }\n",
        "\n",
        "        .stats-container {\n",
        "            display: grid;\n",
        "            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));\n",
        "            gap: 20px;\n",
        "            margin: 30px 0;\n",
        "        }\n",
        "\n",
        "        .stat-card {\n",
        "            background: white;\n",
        "            padding: 20px;\n",
        "            border-radius: 16px;\n",
        "            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.05);\n",
        "            transition: all 0.3s ease;\n",
        "            opacity: 0;\n",
        "            transform: translateY(20px);\n",
        "        }\n",
        "\n",
        "        .stat-card:hover {\n",
        "            transform: translateY(-5px);\n",
        "            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);\n",
        "        }\n",
        "\n",
        "        .stat-card strong {\n",
        "            display: block;\n",
        "            margin-bottom: 10px;\n",
        "            color: var(--primary);\n",
        "            font-size: 0.9rem;\n",
        "            text-transform: uppercase;\n",
        "            letter-spacing: 1px;\n",
        "        }\n",
        "\n",
        "        .stat-card div {\n",
        "            font-size: 1.2rem;\n",
        "            word-break: break-word;\n",
        "            font-weight: 600;\n",
        "        }\n",
        "\n",
        "        .predictions-container {\n",
        "            background: white;\n",
        "            padding: 25px;\n",
        "            border-radius: 16px;\n",
        "            box-shadow: var(--card-shadow);\n",
        "            margin-top: 30px;\n",
        "            transform: translateY(20px);\n",
        "            opacity: 0;\n",
        "        }\n",
        "\n",
        "        .predictions-container h3 {\n",
        "            color: var(--primary);\n",
        "            margin-bottom: 25px;\n",
        "            font-size: 1.3rem;\n",
        "            text-align: center;\n",
        "        }\n",
        "\n",
        "        .prediction-cards {\n",
        "            display: flex;\n",
        "            justify-content: space-around;\n",
        "            flex-wrap: wrap;\n",
        "            gap: 20px;\n",
        "        }\n",
        "\n",
        "        .prediction-card {\n",
        "            flex: 1;\n",
        "            min-width: 200px;\n",
        "            padding: 20px;\n",
        "            border-radius: 12px;\n",
        "            text-align: center;\n",
        "            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);\n",
        "            transition: all 0.3s ease;\n",
        "        }\n",
        "\n",
        "        .prediction-card:hover {\n",
        "            transform: translateY(-5px);\n",
        "            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.15);\n",
        "        }\n",
        "\n",
        "        .prediction-card.up {\n",
        "            background: linear-gradient(135deg, rgba(39, 174, 96, 0.1), rgba(39, 174, 96, 0.3));\n",
        "            border-left: 4px solid #27ae60;\n",
        "        }\n",
        "\n",
        "        .prediction-card.down {\n",
        "            background: linear-gradient(135deg, rgba(231, 76, 60, 0.1), rgba(231, 76, 60, 0.3));\n",
        "            border-left: 4px solid #e74c3c;\n",
        "        }\n",
        "\n",
        "        .prediction-card .value {\n",
        "            font-size: 2rem;\n",
        "            font-weight: 700;\n",
        "            margin: 10px 0;\n",
        "        }\n",
        "\n",
        "        .prediction-card .label {\n",
        "            font-size: 0.9rem;\n",
        "            opacity: 0.7;\n",
        "            margin-bottom: 5px;\n",
        "        }\n",
        "\n",
        "        .prediction-card.up .value {\n",
        "            color: #27ae60;\n",
        "        }\n",
        "\n",
        "        .prediction-card.down .value {\n",
        "            color: #e74c3c;\n",
        "        }\n",
        "\n",
        "        .prediction-card i {\n",
        "            font-size: 2.5rem;\n",
        "            margin-bottom: 15px;\n",
        "            opacity: 0.8;\n",
        "        }\n",
        "\n",
        "        .prediction-card.up i {\n",
        "            color: #27ae60;\n",
        "        }\n",
        "\n",
        "        .prediction-card.down i {\n",
        "            color: #e74c3c;\n",
        "        }\n",
        "\n",
        "        .indicator-container {\n",
        "            margin-top: 40px;\n",
        "            padding: 20px;\n",
        "            background: white;\n",
        "            border-radius: 16px;\n",
        "            box-shadow: var(--card-shadow);\n",
        "        }\n",
        "\n",
        "        .indicator-header {\n",
        "            display: flex;\n",
        "            justify-content: space-between;\n",
        "            align-items: center;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "\n",
        "        .indicator-header h3 {\n",
        "            color: var(--primary);\n",
        "            font-size: 1.3rem;\n",
        "            margin: 0;\n",
        "        }\n",
        "\n",
        "        .indicator-tabs {\n",
        "            display: flex;\n",
        "            gap: 10px;\n",
        "        }\n",
        "\n",
        "        .tab-btn {\n",
        "            padding: 8px 16px;\n",
        "            border: none;\n",
        "            background: var(--light);\n",
        "            border-radius: 20px;\n",
        "            cursor: pointer;\n",
        "            transition: all 0.3s ease;\n",
        "        }\n",
        "\n",
        "        .tab-btn.active {\n",
        "            background: var(--primary);\n",
        "            color: white;\n",
        "        }\n",
        "\n",
        "        .tab-content {\n",
        "            display: none;\n",
        "        }\n",
        "\n",
        "        .tab-content.active {\n",
        "            display: block;\n",
        "        }\n",
        "\n",
        "        @media (max-width: 768px) {\n",
        "            .app-wrapper {\n",
        "                padding: 20px;\n",
        "            }\n",
        "\n",
        "            .app-header h1 {\n",
        "                font-size: 1.8rem;\n",
        "            }\n",
        "\n",
        "            .stats-container {\n",
        "                grid-template-columns: 1fr;\n",
        "            }\n",
        "\n",
        "            .prediction-cards {\n",
        "                flex-direction: column;\n",
        "            }\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <canvas id=\"bg-canvas\"></canvas>\n",
        "    <div class=\"container\">\n",
        "        <div class=\"app-wrapper\">\n",
        "            <div class=\"app-header\">\n",
        "                <h1>Trading On Trends</h1>\n",
        "                <p>Advanced Stock Sentiment Analysis with AI</p>\n",
        "            </div>\n",
        "\n",
        "            <div class=\"search-box\">\n",
        "                <div class=\"search-container\">\n",
        "                    <input type=\"text\" id=\"ticker\" placeholder=\"Enter stock ticker (e.g., AAPL, MSFT, TSLA)\">\n",
        "                    <button class=\"search-btn\" onclick=\"analyzeStock()\">\n",
        "                        <i class=\"fas fa-search\"></i>\n",
        "                    </button>\n",
        "                </div>\n",
        "            </div>\n",
        "\n",
        "            <div id=\"loading\">\n",
        "                <div class=\"loader\"></div>\n",
        "                <p>Analyzing market data and social sentiment...</p>\n",
        "            </div>\n",
        "\n",
        "            <div id=\"results\">\n",
        "                <div class=\"chart-container\">\n",
        "                    <div id=\"price-chart\"></div>\n",
        "                </div>\n",
        "\n",
        "                <div class=\"chart-container\">\n",
        "                    <div id=\"volume-chart\"></div>\n",
        "                </div>\n",
        "\n",
        "                <div id=\"rsi-container\" class=\"chart-container\" style=\"display:none;\">\n",
        "                    <div id=\"rsi-chart\"></div>\n",
        "                </div>\n",
        "\n",
        "                <div id=\"predictions\" class=\"predictions-container\">\n",
        "                    <!-- Predictions will be inserted here -->\n",
        "                </div>\n",
        "\n",
        "                <div id=\"stats\" class=\"stats-container\">\n",
        "                    <!-- Stats will be inserted here -->\n",
        "                </div>\n",
        "\n",
        "                <div class=\"indicator-container\">\n",
        "                    <div class=\"indicator-header\">\n",
        "                        <h3>Technical Indicators</h3>\n",
        "                        <div class=\"indicator-tabs\">\n",
        "                            <button class=\"tab-btn active\" onclick=\"showTab('trend')\">Trend</button>\n",
        "                            <button class=\"tab-btn\" onclick=\"showTab('momentum')\">Momentum</button>\n",
        "                            <button class=\"tab-btn\" onclick=\"showTab('volatility')\">Volatility</button>\n",
        "                        </div>\n",
        "                    </div>\n",
        "\n",
        "                    <div id=\"trend-tab\" class=\"tab-content active\">\n",
        "                        <!-- Trend indicators will be shown here -->\n",
        "                    </div>\n",
        "\n",
        "                    <div id=\"momentum-tab\" class=\"tab-content\">\n",
        "                        <!-- Momentum indicators will be shown here -->\n",
        "                    </div>\n",
        "\n",
        "                    <div id=\"volatility-tab\" class=\"tab-content\">\n",
        "                        <!-- Volatility indicators will be shown here -->\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        // Three.js background\n",
        "        let scene, camera, renderer;\n",
        "        let particles;\n",
        "\n",
        "        function initThreeJS() {\n",
        "            scene = new THREE.Scene();\n",
        "            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\n",
        "\n",
        "            renderer = new THREE.WebGLRenderer({\n",
        "                canvas: document.querySelector('#bg-canvas'),\n",
        "                antialias: true,\n",
        "                alpha: true\n",
        "            });\n",
        "            renderer.setPixelRatio(window.devicePixelRatio);\n",
        "            renderer.setSize(window.innerWidth, window.innerHeight);\n",
        "\n",
        "            camera.position.z = 30;\n",
        "\n",
        "            // Create particles\n",
        "            const particlesGeometry = new THREE.BufferGeometry();\n",
        "            const particlesCount = 2000;\n",
        "\n",
        "            const posArray = new Float32Array(particlesCount * 3);\n",
        "\n",
        "            for(let i = 0; i < particlesCount * 3; i++) {\n",
        "                posArray[i] = (Math.random() - 0.5) * 100;\n",
        "            }\n",
        "\n",
        "            particlesGeometry.setAttribute('position', new THREE.BufferAttribute(posArray, 3));\n",
        "\n",
        "            const particlesMaterial = new THREE.PointsMaterial({\n",
        "                size: 0.2,\n",
        "                color: '#6c5ce7',\n",
        "                transparent: true,\n",
        "                opacity: 0.8\n",
        "            });\n",
        "\n",
        "            particles = new THREE.Points(particlesGeometry, particlesMaterial);\n",
        "            scene.add(particles);\n",
        "\n",
        "            window.addEventListener('resize', () => {\n",
        "                camera.aspect = window.innerWidth / window.innerHeight;\n",
        "                camera.updateProjectionMatrix();\n",
        "                renderer.setSize(window.innerWidth, window.innerHeight);\n",
        "            });\n",
        "\n",
        "            animate();\n",
        "        }\n",
        "\n",
        "        function animate() {\n",
        "            requestAnimationFrame(animate);\n",
        "\n",
        "            particles.rotation.x += 0.0005;\n",
        "            particles.rotation.y += 0.0005;\n",
        "\n",
        "            renderer.render(scene, camera);\n",
        "        }\n",
        "\n",
        "        // Initialize animations\n",
        "        function initAnimations() {\n",
        "            gsap.to('.app-header', {\n",
        "                opacity: 1,\n",
        "                y: 0,\n",
        "                duration: 1,\n",
        "                ease: 'power3.out'\n",
        "            });\n",
        "\n",
        "            gsap.to('.search-box', {\n",
        "                opacity: 1,\n",
        "                y: 0,\n",
        "                duration: 1,\n",
        "                delay: 0.3,\n",
        "                ease: 'power3.out'\n",
        "            });\n",
        "        }\n",
        "\n",
        "        // Initialize app\n",
        "        window.onload = function() {\n",
        "            initThreeJS();\n",
        "            initAnimations();\n",
        "        };\n",
        "\n",
        "        // Tab switching functionality\n",
        "        function showTab(tabName) {\n",
        "            // Update button states\n",
        "            document.querySelectorAll('.tab-btn').forEach(btn => {\n",
        "                btn.classList.remove('active');\n",
        "            });\n",
        "            document.querySelector(`.tab-btn[onclick=\"showTab('${tabName}')\"]`).classList.add('active');\n",
        "\n",
        "            // Update content visibility\n",
        "            document.querySelectorAll('.tab-content').forEach(content => {\n",
        "                content.classList.remove('active');\n",
        "            });\n",
        "            document.getElementById(`${tabName}-tab`).classList.add('active');\n",
        "        }\n",
        "\n",
        "        function analyzeStock() {\n",
        "            const ticker = document.getElementById('ticker').value;\n",
        "            if (!ticker) {\n",
        "                alert('Please enter a ticker symbol');\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            // Show loading indicator\n",
        "            document.getElementById('loading').style.display = 'block';\n",
        "\n",
        "            // Clear previous results\n",
        "            document.getElementById('price-chart').innerHTML = '';\n",
        "            document.getElementById('volume-chart').innerHTML = '';\n",
        "            document.getElementById('rsi-chart').innerHTML = '';\n",
        "            document.getElementById('stats').innerHTML = '';\n",
        "            document.getElementById('predictions').innerHTML = '';\n",
        "            document.getElementById('trend-tab').innerHTML = '';\n",
        "            document.getElementById('momentum-tab').innerHTML = '';\n",
        "            document.getElementById('volatility-tab').innerHTML = '';\n",
        "            document.getElementById('rsi-container').style.display = 'none';\n",
        "\n",
        "            // Hide results container\n",
        "            gsap.to('#results', {\n",
        "                opacity: 0,\n",
        "                y: 30,\n",
        "                duration: 0.5\n",
        "            });\n",
        "\n",
        "            $.ajax({\n",
        "                url: '/analyze',\n",
        "                method: 'POST',\n",
        "                data: { ticker: ticker },\n",
        "                success: function(response) {\n",
        "                    // Hide loading indicator\n",
        "                    document.getElementById('loading').style.display = 'none';\n",
        "\n",
        "                    if (response.error) {\n",
        "                        alert(`Error: ${response.error}`);\n",
        "                        return;\n",
        "                    }\n",
        "\n",
        "                    // Render price chart\n",
        "                    Plotly.newPlot('price-chart', response.price_chart.data, response.price_chart.layout);\n",
        "\n",
        "                    // Render volume chart\n",
        "                    Plotly.newPlot('volume-chart', response.volume_chart.data, response.volume_chart.layout);\n",
        "\n",
        "                    // Render RSI chart if available\n",
        "                    if (response.rsi_chart) {\n",
        "                        document.getElementById('rsi-container').style.display = 'block';\n",
        "                        Plotly.newPlot('rsi-chart', response.rsi_chart.data, response.rsi_chart.layout);\n",
        "                    }\n",
        "\n",
        "                    // Render statistics\n",
        "                    let statsHtml = '';\n",
        "                    const displayableStats = {\n",
        "                        'ticker': 'Symbol',\n",
        "                        'avg_close': 'Avg. Close Price',\n",
        "                        'up_days_pct': 'Up Days %',\n",
        "                        'avg_daily_return': 'Avg. Daily Return %',\n",
        "                        'avg_volume': 'Avg. Volume',\n",
        "                        'max_close': 'Max Close Price',\n",
        "                        'min_close': 'Min Close Price',\n",
        "                        'stddev_daily_return': 'Return Volatility %'\n",
        "                    };\n",
        "\n",
        "                    // Add sentiment stats if available\n",
        "                    if (response.stats.avg_sentiment !== undefined) {\n",
        "                        Object.assign(displayableStats, {\n",
        "                            'avg_sentiment': 'Avg. Sentiment',\n",
        "                            'positive_days_pct': 'Positive Sentiment Days %',\n",
        "                            'avg_posts_per_day': 'Avg. Posts Per Day',\n",
        "                            'sentiment_return_corr': 'Sentiment-Return Correlation'\n",
        "                        });\n",
        "                    }\n",
        "\n",
        "                    // Add technical stats if available\n",
        "                    if (response.stats.avg_rsi !== undefined) {\n",
        "                        Object.assign(displayableStats, {\n",
        "                            'avg_rsi': 'Avg. RSI',\n",
        "                            'overbought_days_pct': 'Overbought Days %',\n",
        "                            'oversold_days_pct': 'Oversold Days %',\n",
        "                            'avg_volatility': 'Avg. Volatility %'\n",
        "                        });\n",
        "                    }\n",
        "\n",
        "                    // Generate stat cards\n",
        "                    let statCount = 0;\n",
        "                    for (const [key, label] of Object.entries(displayableStats)) {\n",
        "                        if (response.stats[key] !== undefined) {\n",
        "                            let value = response.stats[key];\n",
        "\n",
        "                            // Format numbers for better readability\n",
        "                            if (typeof value === 'number') {\n",
        "                                if (key.includes('pct') || key.includes('return') || key.includes('volatility')) {\n",
        "                                    value = value.toFixed(2) + '%';\n",
        "                                } else if (key.includes('volume')) {\n",
        "                                    value = value.toLocaleString();\n",
        "                                } else if (key.includes('sentiment') && !key.includes('days')) {\n",
        "                                    value = value.toFixed(3);\n",
        "                                } else if (key.includes('corr')) {\n",
        "                                    value = value.toFixed(3);\n",
        "                                } else if (key.includes('price') || key.includes('close')) {\n",
        "                                    value = '$' + value.toFixed(2);\n",
        "                                } else {\n",
        "                                    value = value.toFixed(2);\n",
        "                                }\n",
        "                            }\n",
        "\n",
        "                            statsHtml += `\n",
        "                                <div class=\"stat-card stat-${statCount}\">\n",
        "                                    <strong>${label}</strong>\n",
        "                                    <div>${value}</div>\n",
        "                                </div>`;\n",
        "                            statCount++;\n",
        "                        }\n",
        "                    }\n",
        "                    document.getElementById('stats').innerHTML = statsHtml;\n",
        "\n",
        "                    // Render predictions\n",
        "                    if (response.predictions && response.predictions.returns !== undefined) {\n",
        "                        const returnValue = response.predictions.returns;\n",
        "                        const formattedReturn = returnValue.toFixed(2);\n",
        "                        const probabilityValue = (response.predictions.probability_up * 100).toFixed(1);\n",
        "\n",
        "                        let predictionsHtml = `<h3>AI Price Predictions</h3>\n",
        "                        <div class=\"prediction-cards\">\n",
        "                            <div class=\"prediction-card ${returnValue > 0 ? 'up' : 'down'}\">\n",
        "                                <i class=\"fas fa-${returnValue > 0 ? 'chart-line' : 'chart-line fa-flip-vertical'}\"></i>\n",
        "                                <div class=\"label\">Expected Return</div>\n",
        "                                <div class=\"value\">${formattedReturn}%</div>\n",
        "                                <div class=\"confidence\">Based on historical patterns</div>\n",
        "                            </div>\n",
        "                            <div class=\"prediction-card ${probabilityValue > 50 ? 'up' : 'down'}\">\n",
        "                                <i class=\"fas fa-${probabilityValue > 50 ? 'arrow-up' : 'arrow-down'}\"></i>\n",
        "                                <div class=\"label\">Probability of Price Increase</div>\n",
        "                                <div class=\"value\">${probabilityValue}%</div>\n",
        "                                <div class=\"confidence\">Based on market sentiment</div>\n",
        "                            </div>\n",
        "                        </div>`;\n",
        "\n",
        "                        document.getElementById('predictions').innerHTML = predictionsHtml;\n",
        "                    }\n",
        "\n",
        "                    // Render technical indicators\n",
        "                    if (response.stats) {\n",
        "                        // Trend tab\n",
        "                        let trendHtml = '<div style=\"padding: 20px;\">';\n",
        "\n",
        "                        // Moving Average signals\n",
        "                        if (response.stats.avg_close !== undefined) {\n",
        "                            const smaSignal = response.stats.price_to_sma20 > 1 ?\n",
        "                                '<span style=\"color: #27ae60;\">Above SMA (Bullish)</span>' :\n",
        "                                '<span style=\"color: #e74c3c;\">Below SMA (Bearish)</span>';\n",
        "\n",
        "                            trendHtml += `\n",
        "                                <div style=\"margin-bottom: 15px;\">\n",
        "                                    <h4 style=\"margin-bottom: 10px;\">Moving Average Analysis</h4>\n",
        "                                    <p>Current Price to 20-day MA Ratio: ${smaSignal}</p>\n",
        "                                </div>`;\n",
        "                        }\n",
        "\n",
        "                        // MACD signals if available\n",
        "                        if (response.stats.macd_signal !== undefined) {\n",
        "                            const macdSignal = response.stats.macd_diff > 0 ?\n",
        "                                '<span style=\"color: #27ae60;\">MACD Above Signal (Bullish)</span>' :\n",
        "                                '<span style=\"color: #e74c3c;\">MACD Below Signal (Bearish)</span>';\n",
        "\n",
        "                            trendHtml += `\n",
        "                                <div style=\"margin-bottom: 15px;\">\n",
        "                                    <h4 style=\"margin-bottom: 10px;\">MACD Analysis</h4>\n",
        "                                    <p>${macdSignal}</p>\n",
        "                                </div>`;\n",
        "                        }\n",
        "\n",
        "                        trendHtml += '</div>';\n",
        "                        document.getElementById('trend-tab').innerHTML = trendHtml;\n",
        "\n",
        "                        // Momentum tab\n",
        "                        let momentumHtml = '<div style=\"padding: 20px;\">';\n",
        "\n",
        "                        // RSI signals if available\n",
        "                        if (response.stats.avg_rsi !== undefined) {\n",
        "                            let rsiSignal;\n",
        "                            const rsiValue = response.stats.avg_rsi;\n",
        "\n",
        "                            if (rsiValue > 70) {\n",
        "                                rsiSignal = '<span style=\"color: #e74c3c;\">Overbought (Bearish)</span>';\n",
        "                            } else if (rsiValue < 30) {\n",
        "                                rsiSignal = '<span style=\"color: #27ae60;\">Oversold (Bullish)</span>';\n",
        "                            } else {\n",
        "                                rsiSignal = '<span style=\"color: #7f8c8d;\">Neutral</span>';\n",
        "                            }\n",
        "\n",
        "                            momentumHtml += `\n",
        "                                <div style=\"margin-bottom: 15px;\">\n",
        "                                    <h4 style=\"margin-bottom: 10px;\">RSI Analysis</h4>\n",
        "                                    <p>Current RSI(14): ${rsiValue.toFixed(2)} - ${rsiSignal}</p>\n",
        "                                    <div style=\"height: 20px; width: 100%; background: linear-gradient(to right, #27ae60, #f1c40f, #e74c3c); border-radius: 10px; margin-top: 10px; position: relative;\">\n",
        "                                        <div style=\"position: absolute; left: ${Math.min(100, Math.max(0, rsiValue))}%; transform: translateX(-50%); top: -15px;\">\n",
        "                                            <i class=\"fas fa-caret-down\" style=\"color: #2c3e50;\"></i>\n",
        "                                        </div>\n",
        "                                        <div style=\"display: flex; justify-content: space-between; margin-top: 25px; color: #7f8c8d; font-size: 0.8rem;\">\n",
        "                                            <span>Oversold</span>\n",
        "                                            <span>Neutral</span>\n",
        "                                            <span>Overbought</span>\n",
        "                                        </div>\n",
        "                                    </div>\n",
        "                                </div>`;\n",
        "                        }\n",
        "\n",
        "                        momentumHtml += '</div>';\n",
        "                        document.getElementById('momentum-tab').innerHTML = momentumHtml;\n",
        "\n",
        "                        // Volatility tab\n",
        "                        let volatilityHtml = '<div style=\"padding: 20px;\">';\n",
        "\n",
        "                        // Volatility signals if available\n",
        "                        if (response.stats.avg_volatility !== undefined) {\n",
        "                            const volatilityValue = response.stats.avg_volatility;\n",
        "                            const volatilityMax = response.stats.max_volatility || volatilityValue * 2;\n",
        "                            const volatilityRatio = (volatilityValue / volatilityMax) * 100;\n",
        "\n",
        "                            volatilityHtml += `\n",
        "                                <div style=\"margin-bottom: 15px;\">\n",
        "                                    <h4 style=\"margin-bottom: 10px;\">Volatility Analysis</h4>\n",
        "                                    <p>Average Daily Volatility: ${volatilityValue.toFixed(2)}%</p>\n",
        "                                    <div style=\"height: 10px; width: 100%; background: #f1f2f6; border-radius: 5px; margin-top: 10px;\">\n",
        "                                        <div style=\"height: 100%; width: ${Math.min(100, volatilityRatio)}%; background: linear-gradient(to right, #6c5ce7, #a29bfe); border-radius: 5px;\"></div>\n",
        "                                    </div>\n",
        "                                </div>`;\n",
        "                        }\n",
        "\n",
        "                        // Bollinger Band signals if available\n",
        "                        if (response.stats.bb_width !== undefined) {\n",
        "                            const bbWidthValue = response.stats.bb_width;\n",
        "\n",
        "                            let bbSignal;\n",
        "                            if (bbWidthValue > 0.1) {\n",
        "                                bbSignal = 'High Volatility Expected';\n",
        "                            } else if (bbWidthValue < 0.05) {\n",
        "                                bbSignal = 'Low Volatility - Breakout Potential';\n",
        "                            } else {\n",
        "                                bbSignal = 'Normal Volatility';\n",
        "                            }\n",
        "\n",
        "                            volatilityHtml += `\n",
        "                                <div style=\"margin-bottom: 15px;\">\n",
        "                                    <h4 style=\"margin-bottom: 10px;\">Bollinger Bands</h4>\n",
        "                                    <p>Band Width: ${bbWidthValue.toFixed(3)} - ${bbSignal}</p>\n",
        "                                </div>`;\n",
        "                        }\n",
        "\n",
        "                        volatilityHtml += '</div>';\n",
        "                        document.getElementById('volatility-tab').innerHTML = volatilityHtml;\n",
        "                    }\n",
        "\n",
        "                    // Show results with animation\n",
        "                    gsap.to('#results', {\n",
        "                        opacity: 1,\n",
        "                        y: 0,\n",
        "                        duration: 0.8,\n",
        "                        onComplete: function() {\n",
        "                            // Resize charts to ensure they render correctly\n",
        "                            window.dispatchEvent(new Event('resize'));\n",
        "                        }\n",
        "                    });\n",
        "\n",
        "                    // Animate stats cards\n",
        "                    document.querySelectorAll('.stat-card').forEach((card, index) => {\n",
        "                        gsap.to(card, {\n",
        "                            opacity: 1,\n",
        "                            y: 0,\n",
        "                            duration: 0.5,\n",
        "                            delay: 0.1 * index,\n",
        "                            ease: 'power3.out'\n",
        "                        });\n",
        "                    });\n",
        "\n",
        "                    // Animate predictions\n",
        "                    if (document.getElementById('predictions').innerHTML !== '') {\n",
        "                        gsap.to('#predictions', {\n",
        "                            opacity: 1,\n",
        "                            y: 0,\n",
        "                            duration: 0.5,\n",
        "                            delay: 0.3,\n",
        "                            ease: 'power3.out'\n",
        "                        });\n",
        "                    }\n",
        "                },\n",
        "                error: function(xhr, status, error) {\n",
        "                    // Hide loading indicator\n",
        "                    document.getElementById('loading').style.display = 'none';\n",
        "                    alert(`Error: ${error}`);\n",
        "                }\n",
        "            });\n",
        "        }\n",
        "\n",
        "        // Add mouse parallax effect to particles\n",
        "        document.addEventListener('mousemove', (event) => {\n",
        "            const mouseX = event.clientX / window.innerWidth - 0.5;\n",
        "            const mouseY = event.clientY / window.innerHeight - 0.5;\n",
        "\n",
        "            gsap.to(particles.rotation, {\n",
        "                x: mouseY * 0.5,\n",
        "                y: mouseX * 0.5,\n",
        "                duration: 2\n",
        "            });\n",
        "        });\n",
        "\n",
        "        // Handle Enter key in search box\n",
        "        document.getElementById('ticker').addEventListener('keypress', function(event) {\n",
        "            if (event.key === 'Enter') {\n",
        "                event.preventDefault();\n",
        "                analyzeStock();\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Set up ngrok tunnel\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "    create_template_directory()\n",
        "    # Initialize the analyzer\n",
        "    initialize_analyzer()\n",
        "    # Run the Flask app\n",
        "    app.run()\n"
      ],
      "metadata": {
        "id": "CZLWb-tTV5RY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb7f09e-3ca8-4db9-e979-f524b2b618cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://0fa8-35-204-77-46.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            "VADER sentiment analyzer initialized with financial lexicon\n",
            "Reddit client initialized successfully\n",
            "\n",
            "==================================================\n",
            "Processing AAPL\n",
            "==================================================\n",
            "Fetching stock data for AAPL...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Stock data saved to data/raw/AAPL_stock.csv\n",
            "Fetching Reddit data for AAPL from ['stocks', 'investing', 'wallstreetbets', 'stockmarket']...\n",
            "Searching r/stocks for posts about AAPL...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 posts about AAPL in r/stocks\n",
            "Searching r/investing for posts about AAPL...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 posts about AAPL in r/investing\n",
            "Searching r/wallstreetbets for posts about AAPL...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6 posts about AAPL in r/wallstreetbets\n",
            "Searching r/stockmarket for posts about AAPL...\n",
            "Found 9 posts about AAPL in r/stockmarket\n",
            "✅ Reddit data saved to data/raw/AAPL_reddit.csv\n",
            "✅ Merged data saved to data/processed/AAPL_merged.csv\n",
            "Analyzing data for AAPL...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Analysis visualizations saved to visualizations\n",
            "✅ Statistics saved to data/processed/AAPL_stats.json\n",
            "\n",
            "==================================================\n",
            "Processing MSFT\n",
            "==================================================\n",
            "Fetching stock data for MSFT...\n",
            "✅ Stock data saved to data/raw/MSFT_stock.csv\n",
            "Fetching Reddit data for MSFT from ['stocks', 'investing', 'wallstreetbets', 'stockmarket']...\n",
            "Searching r/stocks for posts about MSFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4 posts about MSFT in r/stocks\n",
            "Searching r/investing for posts about MSFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5 posts about MSFT in r/investing\n",
            "Searching r/wallstreetbets for posts about MSFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 9 posts about MSFT in r/wallstreetbets\n",
            "Searching r/stockmarket for posts about MSFT...\n",
            "Found 10 posts about MSFT in r/stockmarket\n",
            "✅ Reddit data saved to data/raw/MSFT_reddit.csv\n",
            "✅ Merged data saved to data/processed/MSFT_merged.csv\n",
            "Analyzing data for MSFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Analysis visualizations saved to visualizations\n",
            "✅ Statistics saved to data/processed/MSFT_stats.json\n",
            "\n",
            "==================================================\n",
            "Processing TSLA\n",
            "==================================================\n",
            "Fetching stock data for TSLA...\n",
            "✅ Stock data saved to data/raw/TSLA_stock.csv\n",
            "Fetching Reddit data for TSLA from ['stocks', 'investing', 'wallstreetbets', 'stockmarket']...\n",
            "Searching r/stocks for posts about TSLA...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 9 posts about TSLA in r/stocks\n",
            "Searching r/investing for posts about TSLA...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 14 posts about TSLA in r/investing\n",
            "Searching r/wallstreetbets for posts about TSLA...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 23 posts about TSLA in r/wallstreetbets\n",
            "Searching r/stockmarket for posts about TSLA...\n",
            "Found 30 posts about TSLA in r/stockmarket\n",
            "✅ Reddit data saved to data/raw/TSLA_reddit.csv\n",
            "✅ Merged data saved to data/processed/TSLA_merged.csv\n",
            "Analyzing data for TSLA...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Analysis visualizations saved to visualizations\n",
            "✅ Statistics saved to data/processed/TSLA_stats.json\n",
            "\n",
            "==================================================\n",
            "Processing AMZN\n",
            "==================================================\n",
            "Fetching stock data for AMZN...\n",
            "✅ Stock data saved to data/raw/AMZN_stock.csv\n",
            "Fetching Reddit data for AMZN from ['stocks', 'investing', 'wallstreetbets', 'stockmarket']...\n",
            "Searching r/stocks for posts about AMZN...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5 posts about AMZN in r/stocks\n",
            "Searching r/investing for posts about AMZN...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7 posts about AMZN in r/investing\n",
            "Searching r/wallstreetbets for posts about AMZN...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 11 posts about AMZN in r/wallstreetbets\n",
            "Searching r/stockmarket for posts about AMZN...\n",
            "Found 12 posts about AMZN in r/stockmarket\n",
            "✅ Reddit data saved to data/raw/AMZN_reddit.csv\n",
            "✅ Merged data saved to data/processed/AMZN_merged.csv\n",
            "Analyzing data for AMZN...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Analysis visualizations saved to visualizations\n",
            "✅ Statistics saved to data/processed/AMZN_stats.json\n",
            "\n",
            "==================================================\n",
            "Processing NVDA\n",
            "==================================================\n",
            "Fetching stock data for NVDA...\n",
            "✅ Stock data saved to data/raw/NVDA_stock.csv\n",
            "Fetching Reddit data for NVDA from ['stocks', 'investing', 'wallstreetbets', 'stockmarket']...\n",
            "Searching r/stocks for posts about NVDA...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8 posts about NVDA in r/stocks\n",
            "Searching r/investing for posts about NVDA...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 9 posts about NVDA in r/investing\n",
            "Searching r/wallstreetbets for posts about NVDA...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 14 posts about NVDA in r/wallstreetbets\n",
            "Searching r/stockmarket for posts about NVDA...\n",
            "Found 18 posts about NVDA in r/stockmarket\n",
            "✅ Reddit data saved to data/raw/NVDA_reddit.csv\n",
            "✅ Merged data saved to data/processed/NVDA_merged.csv\n",
            "Analyzing data for NVDA...\n",
            "✅ Analysis visualizations saved to visualizations\n",
            "✅ Statistics saved to data/processed/NVDA_stats.json\n",
            "Training models using data from: ['AAPL', 'MSFT', 'TSLA', 'AMZN', 'NVDA']\n",
            "Skipping non-numeric feature: ticker_AAPL\n",
            "Skipping non-numeric feature: ticker_AMZN\n",
            "Skipping non-numeric feature: ticker_MSFT\n",
            "Skipping non-numeric feature: ticker_NVDA\n",
            "Skipping non-numeric feature: ticker_TSLA\n",
            "Training data shape: (125, 44)\n",
            "Testing data shape: (25, 44)\n",
            "Performing feature selection...\n",
            "\n",
            "Top 10 features by importance:\n",
            "atr: 0.0543\n",
            "gap: 0.0520\n",
            "Volume_lag2: 0.0494\n",
            "daily_return_lag2: 0.0468\n",
            "daily_return_lag1: 0.0421\n",
            "close_pct_change: 0.0410\n",
            "volume_change: 0.0403\n",
            "Close_lag2: 0.0389\n",
            "close_to_open: 0.0355\n",
            "volatility_daily: 0.0346\n",
            "Selected 30 of 44 features\n",
            "Class distribution: [59 66], balance ratio: 0.89\n",
            "\n",
            "Training regression model for stock returns...\n",
            "Ridge regression R² score: 0.9999\n",
            "\n",
            "=== Regression Model Evaluation ===\n",
            "RMSE: 0.0464\n",
            "MAE: 0.0298\n",
            "R²: 0.9999\n",
            "Directional Accuracy: 1.0000\n",
            "\n",
            "Top 10 features by importance (Regression model):\n",
            "              Feature  Importance  Coefficient\n",
            "24              Close    3.657283     3.657283\n",
            "22             sma_10    0.739206     0.739206\n",
            "25             ema_10    0.572231     0.572231\n",
            "14             rsi_14    0.068908     0.068908\n",
            "27     price_to_sma20    0.064361    -0.064361\n",
            "2         Volume_lag2    0.061123    -0.061123\n",
            "3   daily_return_lag2    0.058424     0.058424\n",
            "29             sma_20    0.057786    -0.057786\n",
            "18              ema_5    0.042597    -0.042597\n",
            "26     sentiment_lag2    0.038808    -0.038808\n",
            "\n",
            "Training classification model for price direction...\n",
            "Logistic regression accuracy: 0.7120\n",
            "\n",
            "=== Classification Model Evaluation ===\n",
            "Accuracy: 0.6400\n",
            "Precision: 0.7500\n",
            "Recall: 0.4615\n",
            "F1 Score: 0.5714\n",
            "AUC-ROC: 0.7051\n",
            "Confusion Matrix:\n",
            "[[10  2]\n",
            " [ 7  6]]\n",
            "\n",
            "Top 10 features by importance (Classification model):\n",
            "              Feature  Importance  Coefficient\n",
            "4   daily_return_lag1    1.031726    -1.031726\n",
            "8       close_to_open    0.763756     0.763756\n",
            "28               High    0.660680     0.660680\n",
            "1                 gap    0.629200    -0.629200\n",
            "10        Volume_lag1    0.599662    -0.599662\n",
            "26     sentiment_lag2    0.550071     0.550071\n",
            "21         Close_lag1    0.533141     0.533141\n",
            "12              roc_5    0.517862    -0.517862\n",
            "25             ema_10    0.387246     0.387246\n",
            "19                Low    0.367686    -0.367686\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-dff04b7511f4>:2673: FutureWarning:\n",
            "\n",
            "\n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "\n",
            "<ipython-input-6-dff04b7511f4>:2673: FutureWarning:\n",
            "\n",
            "\n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Models and metrics saved to models\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://0fa8-35-204-77-46.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 20:24:40] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 20:24:41] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 20:25:06] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 20:25:06] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 20:25:11] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 21:05:32] \"GET / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing analysis request for AAPL\n",
            "Fetching stock data for AAPL...\n",
            "✅ Stock data saved to data/raw/AAPL_stock.csv\n",
            "Fetching Reddit data for AAPL from ['stocks', 'investing', 'wallstreetbets', 'stockmarket']...\n",
            "Searching r/stocks for posts about AAPL...\n",
            "Found 2 posts about AAPL in r/stocks\n",
            "Searching r/investing for posts about AAPL...\n",
            "Found 3 posts about AAPL in r/investing\n",
            "Searching r/wallstreetbets for posts about AAPL...\n",
            "Found 6 posts about AAPL in r/wallstreetbets\n",
            "Searching r/stockmarket for posts about AAPL...\n",
            "Found 9 posts about AAPL in r/stockmarket\n",
            "✅ Reddit data saved to data/raw/AAPL_reddit.csv\n",
            "✅ Merged data saved to data/processed/AAPL_merged.csv\n",
            "Analyzing data for AAPL...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 21:06:04] \"POST /analyze HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Analysis visualizations saved to visualizations\n",
            "✅ Statistics saved to data/processed/AAPL_stats.json\n",
            "Analysis completed for AAPL\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 21:08:52] \"GET / HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}